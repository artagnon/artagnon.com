<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"
  xmlns:dc="http://purl.org/dc/elements/1.1/">
  <author>
    <name>Ramkumar Ramachandra</name>
  </author>
  <id>https://artagnon.com/feed.xml</id>
  <title>artagnon.com</title>
  <updated>2025-12-22T09:47:25+00:00</updated>
  <entry>
    <id>art/2043</id>
    <link href="art/2043"/>
    <summary>&quot;I&#39;m sorry, but we have to let you go&quot;, she says. &quot;Right. What do I need to ink?&quot;, I respond, prepared. I don&#39;t need this job to pay the bills, and I don&#39;t enjoy it. I&#39;d been contemplating resignation for a while now, but a fat severance cheque can&#39;t hurt. The HR woman hands me a crisp document on her tablet. &quot;Do you know what you&#39;ll do next?&quot;, she asks robotically. My mind wanders, and I flippantly say &quot;Maybe I&#39;ll move to Antarctica and take up flame-throwing&quot;. &quot;Oh, good luck with that&quot;, she says, with a meaningless smile. Another robotic response. The lord wonders why she&#39;s still employed.
I wander around the sterile campus filled with man-child millionaires typing away furiously on their notebooks, one last time. The memory of the insipid lunch I had at the canteen lingers. Looking forward to a good celebratory dinner though. I have a foggy recollection of the templated interviews I had to sit through eight months ago — it&#39;s dizzying to think that I survived here for the better part of a year.
As I head back home on the loop, I carelessly browse some blog entries by people who quit the company. It&#39;s been done to death. Everyone thinks that their reasons are somehow more unique or insightful than those of the others, but they all boil down to one reason: stagnation. They all had dreams of becoming billionaires, but got stuck in middle-management. Of course, the posts carefully leave this detail out — instead they talk about &quot;growth&quot;. They sound like vegetables in a garden competing for sunlight and soil.
A little electric transports me from the loop station to my suburban home. These internet companies had their glory days in the early 2000s — today, they&#39;re just addiction algorithms that keep users hooked onto their platforms, and maximize eyeballs on ads. All of them have sizable neuro departments, doing research on addiction patterns in children and young adults. Work is both unethical and mind-numbing, but it pays really well.
As my mind clears, my link brings up ticket prices to various destinations. Impulsively, I book two one-way tickets to Paris departing the next morning. For the highly educated and cultured, moving countries is never a problem — international travel has really eased up in the last decade. Back in the dark ages, the immigrants would visa-hop until they &quot;settled down&quot;. Today, everyone is an immigrant of some kind — who wants to live in the country they grew up in? It&#39;s half-past four, and I recognize my neighborhood from the mini.
&quot;We&#39;re moving to Paris tomorrow!&quot;, I announce, as I get in. Madeline makes a weird anime expression as she looks up from her paperback. I try to identify the soft indie music from the LP, but no avail. &quot;It&#39;s about time&quot;, she says, after a pause, with a broad unquestioning smile. I&#39;d never consulted her on the destination, but she&#39;s not one to take issue with such things. We love surprising each other.
I&#39;m trying to remember how I ever convinced her to move to this suburb in the first place — ah yes, a long vacation in a house facing the sea, with abundant money to buy books, LPs, and scotch didn&#39;t sound like a bad prospect. The last month or so has been a bit hard on us though — the cultural poverty of the surroundings was getting a bit much. There isn&#39;t much to do when friends from other parts of the world visit us. Much of the culture in the nearby city is centered around tech, a topic that we have little interest in. Our friends are initially fascinated by the high-tech convenience systems that come with wealth, but its novelty starts to wear off pretty quickly.
&quot;Mauzac or Négrette?&quot;, she offers, as she gently gets up from the couch. I pick Mauzac, switch the music to Tame Impala, and turn the volume up a notch. Normal people wouldn&#39;t bother with analog music, and just shuffle about within their recommendation systems. &quot;Ancient music!&quot;, she exclaims, and we kiss tenderly and hug as two young lesbians. She never asks about work, and I have no reason to talk about it either. I pull up some AirBnBs on my link, narrow it down to a few near the center of the city, and let Madeline pick one. I&#39;ve already made the big decision of the evening.
&quot;You will get that link removed, yes?&quot;, Madeline asks with raised eyebrows. &quot;I don&#39;t know; it&#39;s so convenient&quot;, I respond sheepishly. It won&#39;t be half as useful in Paris, but I hadn&#39;t given the issue much thought. She shakes her head with a grin. &quot;You resisted getting it for the longest time, but this tech job finally broke you&quot;. We were among the minority in London for resisting links. Madeline thinks it&#39;s an outrageous breach of privacy. I agreed with her for the longest time, but fell into the convenience-trap in the end.
The doorbell chimes, indicating that our bot has arrived with packing boxes. Moving is hardly an ordeal these days — we move every three or four years. Unlike most people, we don&#39;t need a reason to move, but rather a reason not to move. Becoming fluent in the local language usually doesn&#39;t take us more than a couple of months; Mandarin was the exception — took us a little over six. Prior to our move to this suburb, we had four good years in London. She was working at the local museum, and I was doing my PhD in higher categories.
&quot;Will you miss the high-tech conveniences?&quot;, she asks naughtily, as the sophomore girlfriend. She loves doing this to the rigid old man: poking him about what he&#39;ll miss everytime something changes. When we moved from London, she asked me whether I&#39;d miss the tube. &quot;I certainly won&#39;t miss the 8-hour weekday grind&quot;. I was exaggerating — having expended the minimum work in order to keep my dayjob afloat, and used a lot of my time to read and practice oil-on-canvas. These goliath companies are complacent about keeping tabs on their employees, as most of them are addicted to developing the addiction algorithms. The irony. Nevertheless, that should change the line of conversation.
We have a few baseline characteristics, and keep transforming ourselves with every move. &quot;In Madrid, we were true Spanish, in London, we were true Londonders, but I don&#39;t know what we became in this suburb&quot;, she continues after my thought. The suburbians are poorly-traveled boring people with straight career-lines whose dominant interest is children. Designing children. Even the most conservative ones genetically engineer atleast the hair color and skin tone. &quot;Culturally poor, incredibly wealthy burbians, but without kids?&quot;, I offer, in summary. &quot;You remember when our lovely neighbors tried to come over during the first week and offer us their wonderful food?&quot;, she asks sarcastically. I raise my eyebrows and shut my eyes in feigned embarrassment. We had to throw away most of the food because it was so insipid.
&quot;I&#39;ll let you pick dinner&quot;, I state with authority, playing the dominant role. &quot;Bœf bourguignon with cream of artichoke?&quot;, she offers coyly. As we move into the kitchen, the bot skirts past us — it&#39;s finished with the study, and is heading for the bookshelf in the living room now. We&#39;re always a little disappointed when we open the refrigerator. There is literally one weekend market within commuting distance of where we live, where we have to get our ingredients for the week. It&#39;s overpriced, and the stuff isn&#39;t great, and there&#39;s very little diversity. We can&#39;t stand the frozen stuff everyone else gets at these gigamarkets. London had a terrible food culture as well: everyone was getting takeout boxes from the local Indian restaurant. &quot;Don&#39;t fret. We&#39;ll soon be heading to the local butcherie, poissonnerie, and fruit/légume marché every other day, just like in Hong Kong&quot;, I say with a broad smile.
As I cleave the beef, she expertly chops the carrots and peels the potatoes while sipping her wine. Many of these burbians get their ingredients shipped home, and have bots cook for them, an idea we find repulsive. &quot;Will you have something to read on the flight tomorrow?&quot;, I ask with the concern of a close girlfriend. We both read rapidly, but she&#39;s got to a different level during the course of this vacation; burns through two or three paperbacks a day. &quot;You&#39;re nearly done with Daisy Johnson&#39;s latest, yes?&quot;, I ask referring to the book she was reading when I entered. She nods, and we both rush to the living room, to pick a couple of novels set in Paris from the shelf before the bot gets to them. I love my paper books as well; vastly prefer them to reading off the screen or the link.
As the prep phase of the cooking draws to an end, the induction top switches on. In Hong Kong, we used to have a traditional gas top, and often cooked in woks. &quot;We don&#39;t have to worry about earning for a while now&quot;, she remarks. It&#39;s true; with our savings and the generous UBI pay in the EU, why would anyone work for a living? &quot;The last time we had that luxury was in Madrid&quot;, I respond, with a tinge of bitterness. &quot;I&#39;ll probably write a novel&quot;, she says. She&#39;s only written short stories so far, and this stint has more than prepared her to write her first novel. &quot;And where is it going to be set?&quot;, I ask flatly, as the professor. &quot;Paris, of course. Modern era&quot;, she says with a beaming smile. &quot;themed in European film history&quot;, she continues. She&#39;d have to learn a few major European languages and cultures. &quot;You&#39;re going to be nosed up studying for quite a while then&quot;, I say, downplaying the excitement. She knows that I know that we&#39;re going to be traveling a lot, with purpose, and merely smiles at the unvoiced thought.
The soup is ready, and we start sipping it in the dining room, while the beef simmers. &quot;I&#39;ll work on a project to formalize neuroscience&quot;, I say finally. The question has been lingering for some time now. &quot;Quite the challenge&quot;, she remarks, adjusting her glasses, playing the nerd. The bedrock of most software construction starts with an understanding of the brain. Most software construction and most neuroscience is stuck in the dark ages though. For the most part, Madeline only uses software that is correct by construction. She&#39;s disgusted by recommendation systems, and terrified that a software bug will breach her privacy or hurt her. Practically speaking, this restricts her choice to a subset of pre-neuro software. &quot;I should get my link removed&quot;, I say in sudden alarm. &quot;Yes, you should; the Parisians won&#39;t take well to it&quot;, she responds soberly.
The bourguignon is delicious. Perfect dinner to end this stint. We retire to bed with our books, smoking a couple of cubans.</summary>
    <title>2043</title>
    <updated>2020-09-01T16:54:09+02:00</updated>
    <dc:date>2020-09-01T16:54:09+02:00</dc:date>
  </entry>
  <entry>
    <id>art/aging</id>
    <link href="art/aging"/>
    <summary>I&#39;m eighty today, but the sequence of events I&#39;m going to narrate took place when I was in my early twenties. It was a pleasant Friday in San Francisco, and as many people will tell you, SanFran has the ideal weather. The mood at my workplace was jubilant. Added to the fact that the weekend was coming up, our CTO had announced that we would be going public the following Monday. Everyone was dressed in colorful tees and shorts, and we were all trying to estimate what our net worth would be after the IPO, based on the amount of vested shares each of us had. Needless to say, we were doing very well, and were expecting the IPO to make a big splash. Many of us were dating each other in the office; even our CTO was under thirty. As the afternoon wound down, all of us left early, and in high spirits.
I mindlessly took a stroll along the pier with a co-worker, and we got ice-cream to celebrate. She left shortly after, citing that she wanted to spend time with her boyfriend. People had suspected that there was something off with me, but wrote it off as plain weirdness that I&#39;d eventually grow our of. I was careful not to get too close to anyone, for I knew, deep within myself, that I was pretending. I didn&#39;t belong here, among these people. If you ask a lay person what this meant, they&#39;d ask if the problem was racism, sexism, or some such social issue, but the problem with me was much deeper than that; I constantly felt like a 200 year-old person.
As I continued my stroll back home alone, I entered a deep state of melancholy. Normal people could live their lives in pursuit of wealth, good-looking partners, and social status. I just didn&#39;t get the point. I might be in my early twenties today, but time will fold onto itself until I&#39;m forty, sixty, and eventually eighty. One way to put it, perhaps, would be to say that I couldn&#39;t live in the drama of the searing present. There&#39;d be an IPO, which might bump our net worth and social status. We might be able to afford a 3000 sq-ft home in SanFran as a result. Or our company might sink, and we might have to move to a less fashionable city, in order to buy a house. It was inconsequential to me either way: I just didn&#39;t see the point of thinking about it; what will happen will happen, whether or not we bite our nails over it.
We&#39;re born one day, and we die approximately eighty years later. Society is not structured to accommodate people well beyond their age. That was the truth of what I was doing at a young startup in SanFran: the first-hand life experience, not because I expected to enjoy it, but because it would be an interesting observation to fill the 23rd year of my life. 23, because it was a socially-acceptable age for this.
I felt every groove on my key as I inserted it into my door. I&#39;d already planned my departure, in complete secret. I took off my shoes, and started up my surround-sound system to play soft Icelandic music. Having curled up on the couch comfortably with some good whiskey, I lit up a Cuban cigar. All the creature comforts were attended to; we were all well-paid, and nobody had suspected who I really was. My bookshelf was overflowing with Pulitzer/Booker-prize finalists, and many other rare gems. My e-ink tablet had serious works on mathematics, philosophy, neuroscience, and physics. Normal people usually have some self-help books disguised as psychology, and other baloney disguised as non-fiction. Any serious text would only be read in the context of their jobs.
I started thinking about Reykjavik. My flight was in three hours, and the packing was done; I was departing permanently. I&#39;d spend a good six months relaxing in Iceland, and plan my next steps. I&#39;d sell my stock after the IPO remotely, and give in my resignation (again, via phone) shortly after. The end of my one-year lease on the house was approaching shortly — I&#39;d already asked my landlord to cancel the renewal of my lease, and had arranged for my stuff to be stored in a warehouse via a moving company. I&#39;d packed ten novels to read over the course of six months. I was leaving most unceremoniously.
Was I feeling happy or relieved? Not really. I&#39;d still have to pack the years of my life with stuff to do, until I felt like I&#39;d found home and wanted to put down roots. I wouldn&#39;t want to be a pretending-nomad forever, and perhaps I&#39;ll always be a misfit, but I was sure there&#39;d be a place I&#39;d be able to call home. Time was abundant.
The re-telling of something that happened over fifty years ago brings me great joy. I&#39;m near the end of my life today, and I can whole-heartedly say, that I have lived a full life. The deep sense of melancholy that I had at the age of 23 only deepened with age, but I learnt to live with it. I live in a place where people understand and love me, and for that, I feel very fortunate.</summary>
    <title>Aging</title>
    <updated>2022-04-16T12:43:00+02:00</updated>
    <dc:date>2022-04-16T12:43:00+02:00</dc:date>
  </entry>
  <entry>
    <id>art/ai-war</id>
    <link href="art/ai-war"/>
    <summary>There are innumerable reasons to hate AI, starting with its environmental impact: the AI boom has been estimated to consume as much power as New York City, and as much water as the global consumption of bottled water in 2025. The reasons could also be personal, from leading to a loss of livelihood, to threatening the way we pursue our passions. The distaste is fueled by visible misuse, including the flood of low-quality AI-generated bug reports and PRs on open-source projects, music and books uploaded to Spotify and Amazon, papers at academic venues, and the &lt;a href=&quot;https://youtu.be/E-YwjXEVGo8?si=RcEymmyF_qWB13ks&quot;&gt;recent horrifying McDonalds ad&lt;/a&gt;. To make things worse, there is plenty of &lt;a href=&quot;https://dmitrybrant.com/2025/09/07/using-claude-code-to-modernize-a-25-year-old-kernel-driver&quot;&gt;anecdotal evidence&lt;/a&gt; to suggest that AI makes an individual more productive, while there are plenty of &lt;a href=&quot;https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/&quot;&gt;studies&lt;/a&gt; that suggest the opposite. At the same time, there is no denying that the technology is &lt;em&gt;very&lt;/em&gt; widespread across the industry, well beyond just the software industry. For many of us, personal distaste and ethical issues has lead to a AI-denialism stance, where we &lt;a href=&quot;https://mikelovesrobots.substack.com/p/wheres-the-shovelware-why-ai-coding&quot;&gt;declare that AI doesn&#39;t work for anyone&lt;/a&gt;, and espouse &lt;a href=&quot;https://pluralistic.net/2025/12/05/pop-that-bubble/&quot;&gt;borderline conspiracy theories&lt;/a&gt;.
The primary feature of AI to pay attention to is that it&#39;s a fuzzy non-deterministic system, and measuring productivity gains from AI at an individual level is a bit like studying witchcraft. Today, AI adoption across the industry — be it software, finance, digital humanities, law, journalism, or otherwise — is so widespread, that dismissing it as just &quot;hype&quot; or &quot;herd mentality&quot; is not a cogent position any longer. To a first approximation, corporations are rational capitalist actors looking to maximize their profits, and they are paying good money for AI products for a reason. I only see one conclusion by Occam&#39;s razor: that, for many orgs, implementing AI org-wide is a net-positive, although the results vary at an individual-level. Many orgs have run internal productivity metrics and found a 20-30% increase — and the proof is that they continue paying for AI products, even in the light of studies that suggest that AI makes individuals less productive. This would lead us to look for publicly observable metrics to correlate the fact that there&#39;s an increase in productivity across the board: for example, we might expect to see quicker software release cycles, an uptick in feature rollout or bug-fixes in products, or in open-source code. Perhaps not the not the most satisfying answer, but orgs are not increasing their production volume with AI adoption, and are instead raking in profits by lowering their costs — and the publicly observable metrics are the layoffs, hiring slump, unemployment, and under-employment in advanced economies. The other problem is that the technology is a poor fit for open-source projects, and I will expand on this next. As an economist put quite poignantly, the current situation is akin to a &lt;em&gt;war-time economy&lt;/em&gt;, where the defense companies are raking in profits with huge environmental costs (or the AI adopters today), while everyone else is left to survive on rations.
Open source communities work because the community members are deeply passionate about the project, and treat their work like a hobby — they take pride in their work, and there is nothing to sell. All open source projects must attract and retain new contributors every year to survive. Maintainers often do a lot of work helping new contributors, and the payoff is only seen when a new contributor continues gaining experience, and eventually grows into a maintainer themselves. This key social aspect of an open source project is the reason why AI adoption will be minimal at best: the cost of maintaining LLM-generated code outweighs the marginal benefit of adding a new feature — and since it was low-effort for the new contributor, they are less likely to stay on and learn. The cost-benefit computation is different for a corporation selling a product — revenue is directly correlated to feature rollout. The idea of corporations profiting off open source is nothing new — and in this age, all they have to do is train their LLM on our hand-crafted &quot;artisanal&quot; code.
The economics of AI &lt;a href=&quot;https://www.wheresyoured.at/the-case-against-generative-ai/&quot;&gt;doesn&#39;t add up at all&lt;/a&gt;, and the technology cannot even be sold at cost — there is no denying that we live in a bubble similar to the sub-prime mortgage bubble. After the crash, a lot of everyday people will lose a significant portion of their savings for no fault of theirs, and what will probably be left are cheap compressed versions of what we have today, running on commodity hardware.
We have been very irresponsible in going down this AI path — and we&#39;re doing irreversible damage to the planet while at it. I hate AI for the reasons outlined, and it is useless to me, as a member of the LLVM community. However, I cannot deny that the technology is widely useful and has increased the productivity of many orgs.
That said, there are also many corporations that have adopted AI with no material benefit.
Perhaps the long-term effect on corporations that have adopted AI will be the accumulation of technical debt, leading to shorter-lived corporations.</summary>
    <title>AI and the war-time economy</title>
    <updated>2025-12-20T13:18:05+00:00</updated>
    <dc:date>2025-12-20T13:18:05+00:00</dc:date>
  </entry>
  <entry>
    <id>art/banter</id>
    <link href="art/banter"/>
    <summary>My heart started racing the moment I saw her. She was standing right there by the coffee machine. The first thing I noticed about her was her hair. Curly hair, that was neatly combed back, with a couple of ornamental clips, pinned haphazardly. Neatly trimmed in line at the neck. She was short, had soft glowing skin, and was speaking very softly, in Russian, with another Russian girl who worked there, that I was acquainted with. She had beautiful legs, dressed in tight jeans, and she was wearing a beautiful red top that day. Exceptionally well-groomed and fit. This was a woman who took great care of herself. In short, stunningly beautiful. I walked rapidly up to the coffee machine, not because I was in a hurry to meet her, but because I always walked rapidly.
&quot;Is the coffee machine broken again?&quot;, I asked. &quot;Oh, it just needs some love and taking care of, like a cat.&quot; &quot;Purr, Purr&quot;, she whispered, as she stroked the coffee machine. I started laughing immediately, and without inhibition. I&#39;m a tall well-built French man, and I was dressed in a plain black North Face tee and cargos. Since my friend had introduced me to it, I always wore jet black Adidas boots, and that always helped making a big entrance.
She was smiling gently at me. Terribly shy, I could tell. &quot;See, it&#39;s fixed now?&quot;, she said, pointing to her full cup of black coffee. &quot;Do you work here?&quot;, I asked gently. &quot;Yes, I&#39;m Irena&quot;, she responded, gently turning away in shyness. I recalled that there was an office on our floor with the name Irena that was unoccupied. People didn&#39;t talk about her much at work, but I knew that she&#39;d been on vacation. &quot;I&#39;m Etienne. It&#39;s nice to finally meet you.&quot; &quot;So, you&#39;re back from vacation?&quot;, I said trying to hide the eagerness in my voice. &quot;Yeah, you know vacations are like that&quot;, she said, feigning boredom. She then collected her coffee, and returned to her office with Katina, the other Russian girl she was with. It was post-lunch, and we, as the French, have long lunch breaks. Irena was in the same team as me, and we&#39;d be meeting each other multiple times over the coming months, over multiple lunches and coffee breaks. I was mostly just looking forward to the next day.
There was a talk at IRIF that day, that one of my team members was delivering. I spotted Irena sitting in the front row, feeling uncomfortable because Katina wasn&#39;t by her side. Then, as Katina entered the room, Irena waved and tried to get her attention. It was then that she noticed me sitting in the rear, and staring at this amusing ordeal. I merely smiled, but she immediately turned away, embarrassed, and stop waving.
The very next day, I noticed thumping footsteps outside my office, and realized that both Irena and Katina had taken to wearing boots.
Over the next couple of months, our relationship into light casual banter over coffee and lunch. Then, one day, Irena decided to sit next to me at lunch, with Katina by her side. We had a joyful conversation on Master and Margarita. At this point, she decided to sit next to me everyday over all the lunch sessions over the coming months, but for the occasions on which she was cross with me. We were definitely more fond of each other than with the others on the team.
It was my morning routine. Waking up at 5a, and arriving in the office before everyone else, occupying the lounge, and studying mathematics on my tablet. I would wait for Irena to arrive at around 8a, and get my fourth or fifth morning coffee with her, with nobody else watching, and not even Katina by her side. After that, I&#39;d walk to the nearby bakery and get a croissant aux amande, my usual breakfast. On that particular day, I was feeling particularly hungry, and Irena was probably running a little late. I decided to get the croissant myself, and hoped to catch Irena afterward. As I took the stairs to the ground floor, we noticed each other. She had just entered the elevator, and I was walking past it. At this point, she lowered her glasses, and looked at me, definitely feeling a little cross for not having waited for her.
I missed work on three consecutive days, because I had a bout of food poisoning. The next day, when I turned up at work, Irena turned up to get her her coffee, with ruffled hair. She looked distressed, and it seemed to be because she had been missing me. &quot;Where have you been?&quot;, she opened with. &quot;Oh, you know. Food poisoning.&quot; &quot;Sorry I missed Katina&#39;s birthday.&quot; &quot;Oh, it&#39;s alright, I&#39;m glad you&#39;re fine.&quot;, she responded. After the usual morning coffee, and somewhat subdued banter, she returned to her office. Moments later, it occurred to me that I could have brought up her hair, so I quietly tiptoed to her office, and peered in. She seemed to have combed her hair at this point, and was no longer looking distressed.
I grew really fond of Irena over the next few months. After carefully evaluating I ought to proceed, I decided to gift her a book. Since I knew her taste in reading at this point, I inked a freshly purchased copy of Vulcan&#39;s Hammer with my beautiful cursive handwriting in blue fountain-pen ink. A slightly personal &quot;To Irena, the Fencing-Salsa-Japanese girl, from Etienne&quot; is what I inked, recalling her three primary hobbies.
I wore my favorite shirt to work that day, with excitement. When she arrived for her morning coffee, I opened with &quot;I have a gift for you&quot;, and presented her the book. She panicked, and in an attempt to downplay it, she asked me to wait for just a second, rushed to her office, found a copy of an old Systems Engineering textbook, presented it to me and said &quot;So, it&#39;s a book exchange&quot;, with a serious expression. I smiled politely, disappointed.</summary>
    <title>Banter</title>
    <updated>2021-09-05T14:16:39+02:00</updated>
    <dc:date>2021-09-05T14:16:39+02:00</dc:date>
  </entry>
  <entry>
    <id>art/bhutan</id>
    <link href="art/bhutan"/>
    <summary>I woke up feeling like I had a brand new body and mind. We were asleep in a little cabin in the middle of nowhere, in a little village in Bhutan. We&#39;d arrived the previous day, by air, and ridden on top of a bus while jet-lagged to get to find this cabin. We&#39;d all arrived from different countries: Sven from Svalbard, Aamod from Brooklyn, and I&#39;d made it from Montpellier. To add to the chaos of our flights arriving at different times, we had no mobile connectivity in the Thimphu airport, and it took the better part of the previous day to find each other roaming around randomly in various tea shops. There was no plan, we hadn&#39;t booked anything in advance, and none of us knew Dzongkha. Yet, somehow, with Aamod&#39;s broken Hindi-Urdu, we decided to take a random bus going to an unknown destination, only to find that the bus was full. That&#39;s the story of how we ended up on top of the bus. Alright, there was one crucial preparatory step I&#39;m skipping here: we&#39;d gone to our respective banks and exchanged our currencies for Indian rupees, since no bank had heard of Ngultrum; we&#39;d read online that Indian rupees would work in Bhutan. With a limited supply of these Indian rupees in hand, we would manage our expenses over our week-long vacation. When we got off the bus, we found ourselves literally in the middle of nowhere, and trekked a bit until we found a house with a family staying in it. They were very welcoming to us, served us tea, and put us up in their cabin for a small sum.
Aamod was already awake, and gently strumming his ukulele, while Sven was slowly waking up. &quot;Bonjour, Hugo&quot;, he sang with a broad smile. &quot;Bonjour!&quot;, I responded, cheerfully. Sven had started stirring now, and was nearly awake. We were three very unlikely friends, with very different personalities and vocations. Sven was a jazz musician, Aamod was a student in linguistics and sociology, and I was a mathematician. As an upcoming musician in Svalbard, Sven was the person who was most in touch with nature, and the least in touch with people. Aamod, on the other hand, was a very sociable person, and we were relying on him to handle most of the logistical challenges of this trip. I was a soft-spoken mathematician who had an adventurous streak, but had no creative talent to speak of. Aamod and I had met in San Fransisco, quite randomly at a pizzeria at 4am. We were immediately drawn toward each other, and spent the following day roaming the streets and smoking weed. We&#39;d decided to go to Rejavik the following summer, without a plan, and found Sven playing jazz at a bar there. We loved Sven&#39;s music, and spent the better part of that week with him, skinny-dipping in the ice-cold waters, and enjoying the sounds of nature. To be honest, we didn&#39;t know each other very well, but both Sven and I were immediately enthusiastic when Aamod proposed that we go backpacking in Bhutan. Many years down the line, we would become close friends, and we&#39;d realize that this week spent together in Bhutan would establish a very strong bond between us.
&quot;Hej, Sven!&quot;, I chimed, as Sven sat up in his bed, with a start. &quot;Morning, both. I need some tea&quot;, he responded. Aamod stopped strumming, and started scratching his unruly beard. Sven was a heavy coffee drinker, and he was slowly getting accustomed to the fact that only tea would be available in Bhutan. &quot;Get dressed, then. We&#39;ll head over to the house — they&#39;ll be happy to see us up so early&quot;, I said. Indeed, my watch read 6:30a, and we hadn&#39;t gone to sleep until 10 the previous day; yet, we all felt like we&#39;d been asleep for a week. We all suited up for the cold, and went knocking on our host&#39;s door. The kind old woman who greeted us the previous day appeared at the door and was noticeably surprised. We all smiled and bowed — no words were exchanged. She led us into the large dining area and seated us on the floor. She then automatically proceeded to serve us a milky tea mixed with butter and salt. Sven preferred the milky tea with sugar, and Aamod and I smiled in amusement as he sipped his tea with some discomfort. &quot;You&#39;ll get used to it — it&#39;s a good drink for the cold weather&quot;, said Aamod, gently patting him on his shoulder. When we&#39;d all finished the tea, Sven managed to utter something that sounded like &quot;Dhanyavaad&quot; to the woman, emulating the sound Aamod was constantly making the previous day, to convey his gratitude; this was a warm surprise. The woman smiled generously, and disappeared into the kitchen again to fetch breakfast. When the plates were placed before us, we recognized the food as some sort of barbecued meat along with some boiled and salted peas, with the skin still on. After taking a bite, Aamod and I couldn&#39;t recognize the meat; Sven noticed our confusion, and said &quot;Oh, it&#39;s really tough sheep meat&quot;.
We were eager to get to a nearby village, where we could meet some people, so we caught another random bus from the same spot as which we&#39;d gotten off the previous day. Thankfully, we were riding inside the bus this time, and we kept a lookout for small towns. Once we reached a small town, we got off, and roamed around a bit observing the people and the the unpaved streets. People seemed to be observing us with curiosity, and some of them walked up to us, and asked us where we were from. Aamod is mixed-race Indian-American, Sven is Nordic, and I&#39;m German-French. Sven looks especially curious with his prominent tattoos, facial piercings, long hair, and long well-maintained beard. I have a typical mathematician&#39;s beard, and Aamod has an especially unruly one. Neither of us have any visible tattoos or piercings. We were all wearing some kind of jacket, which in itself was enough to make us stand out: everyone there was dressed in some kind of indigenous woolen top garment. There were very few automobiles, and even those were motorcycles. The town wasn&#39;t electrified of course, although we did spot some diesel generators at the shops.
I&#39;d brought along a dSLR camera, with which I was awkwardly taking pictures. We decided to make camp in a tea shop, whence Aamod and Sven decided to take turns playing the ukulele. Little by little, people from the town started gathering around us, out of curiosity. At one point, Aamod and Sven began singing, in what would be a long performance for the town. Soon, most of the town was at the tea shop, and everyone was brimming with joy.
On the next day, we headed to a garment shop, and spent the entire morning curiously looking at the various kinds of garments. I picked a shawl with bright colors on it, while Sven picked something tasteful and expensive, and Aamod settled on a long gray flowing top garment. The reason Sven and I didn&#39;t pick anything other than shawls is because the garments were somewhat ill-fitting: as Europeans, we were tall and well-built with broad shoulders; Aamod didn&#39;t seem to mind the ill-fitting purchase as much though. We did away with our jackets, and got dressed up, excited to see how we&#39;d be perceived now.
As a light late-afternoon snack, we tried some curious yellow rice-based dish, and all three of us were feeling unreasonably sleepy after that. We&#39;d later discover that it was a poppy-based dish, and is meant to be had in the afternoon before a siesta. At this point, we&#39;d found a lake and taken a boat, but none of us was in any condition to swim the cold waters, so we decided to relax for some time in the middle of the lake, and go for a swim another day.
Initially, I was slightly uncomfortable about the fact that we hadn&#39;t talked much — I&#39;d come to realize that we never would say more than a few words to each other on this trip; there was an unspoken agreement between us that we&#39;d leave our lives behind on this vacation.
On the fourth day, we discovered that people here smoked opium for leisure, and both Aamod and I were shocked at the prospect of smoking opium. Sven laughed generously, and convinced us to try it. The host had brought us something called a &quot;chillum&quot;, a sort of long pipe made of stone, with a piece of cork holding the substance the person wants to smoke in place. Then, he took out his handkerchief, and used it as a filter. A very crude setup overall, but it suffices to say that we had a very relaxing evening on the porch of his house. I enjoyed myself so much, in fact, that I&#39;d buy a chillum and take it back with me, as a reminder of that evening.
We decided to spend the last couple of days of our vacation in Thimphu. We&#39;d made a friend, who spoke English. He told us that he was an ex-con, who&#39;d escaped from prison, under the false pretext of insanity. We all laughed together over his wild stories.
When it was time to take our flights back home, we promised each other that we&#39;d plan a longer vacation the next time around. The airport security personnel freaked out a bit when she saw my chillum, mistaking it for some kind of pipe bomb. I calmed her down, and explained to her what the thing she was holding in her hand was.</summary>
    <title>Bhutan</title>
    <updated>2022-01-06T17:02:32+01:00</updated>
    <dc:date>2022-01-06T17:02:32+01:00</dc:date>
  </entry>
  <entry>
    <id>art/insomniac</id>
    <link href="art/insomniac"/>
    <summary>I couldn&#39;t sleep that night. Tired and frustrated, I got out of bed, took a shower, and dressed for the snowy streets of Boston. Thermals, t-shirt, jeans, and a quirky jacket I&#39;d picked up at the thrift store, that I was quite fond of. I started making plans in my head. I&#39;d take the 5:30am train to Natick after finishing breakfast with a Bloody Mary. There was a diner close to my place that was open through the night. The feeling of frustration was fading, and was quickly being overtaken by a feeling of excitement. I was used to this: I often slept poorly, and had the habit of turning up at work at ungodly hours.
As I opened the front door of my building, a gush of cold air blew in. It was 2a, and the streets were empty, covered in a few inches of snow. Perfect timing. I&#39;d probably run into the most random and interesting people. Picking out my first smoke from the pack, I noticed the first human being ambling about. There was no rush to get to the 24-hour diner.
He looked a bit shabby, but was dressed comfortably for the winter. In his mid-40s, I&#39;d say. Definitely not homeless. A little tipsy, he was drinking a Lagunitas, and was carrying a few more with him. &quot;Hey there, you wanna trade some smokes for a beer, mate?&quot;, I said as I approached. &quot;Sure, man. What&#39;re you doing up so late?&quot;, he said, handing me a beer. &quot;I don&#39;t get much sleep anyway. Might as well roam about, am I right?&quot;, I said with a smile, as I handed over three Lucky Strikes. &quot;Bloody hell man. My wife kicked me out, and I&#39;m taking the 7a train to Washington.&quot; I suspected as much. &quot;Tough luck, mate. No luggage, eh?&quot;, I said, with a little sympathy. &quot;No, man. Didn&#39;t have enough time to pack. Gonna visit my sis.&quot; &quot;I&#39;m heading to South Street to get breakfast. Cherios.&quot;, I said, ending the conversation with a wave.
I was already feeling the rush of being out in the cold on no sleep. I&#39;d make the most of my walk to the diner, I thought to myself, as I plugged into my earphones. Bowie, I figured, as I shuffled through my Spotify. It would be a long and satisfying walk, I thought to myself, as I took another swing of the Lagunitas. I&#39;d have some time to think about breakfast, and the new compiler transform I was developing at work. I spotted few other people, who didn&#39;t seem to be interested in picking up conversation. There were few homeless people in this part of the world, as the winters were so inhospitable.
As I approached the diner, I unplugged my earphones, and noticed a bunch of people outside smoking and chatting. I loved South Street, especially at odd hours, because it had such weird mix of people: people working night-shifts, strippers, the unemployed, and people who had trouble sleeping. I took a seat at the bar next to a goth girl, and started scanning the menu for items I&#39;d not tried in the past. The diner was cheap, and the food was decent. And, as was the case with all diners, bottomless coffee. It was always lively, but never noisy. This was probably the only 24-hour diner in Boston, and I lived close to it. I felt so lucky.
Within thirty seconds of taking a seat at the bar, the waiter had filled my mug with fresh coffee. &quot;What&#39;ll ya have?&quot;, she said, in a completely no-nonsense manner. &quot;I&#39;ll have the scrambled eggs and bacon, with a side of sausage. And a Bloody Mary to finish off.&quot; I&#39;d ordered the usual, realizing that I&#39;d tried everything on the menu already. She made a quick note, and concluded with a &quot;Comin&#39; right up.&quot; Although I visited this place a lot, I wasn&#39;t acquainted with the waiters. They had a lot of customers, and were constantly working. There was nothing special about me. &quot;Come here often?&quot;, I quipped, turning to the goth girl. She had tattoos all over, and was dressed in black overalls. &quot;Yeah, it&#39;s my jam.&quot;, she said, turning away. She didn&#39;t seem to be interested in any further conversation, so I dropped it.
The bacon was tasty, as were the sausages. The servings were all generous, and I was content with my heavy breakfast. The Bloody Mary had just the right amount of salt and celery, and I took my time to finish. I paid by card, and left without saying another word. I was probably the most square person at South Street, if you were to judge by appearances: I didn&#39;t have a single tattoo or piercing, and was employed in a well-paid software company. Every now and then, I&#39;d pick up a short conversation with someone there, but in general, I had little in common with the people there. Yet, I enjoyed their presence.
I was really enjoying the mix of the Mary and nicotine, as I noticed the first rays of light from dawn streaming into the streets. As I approached the train station, I noticed an aging woman from the distance. My watch read 5a, and I&#39;d packed a book to read on the station and train, but she seemed to be a little distressed. As she approached me, I realized that the winter air must be a little harsh on her. &quot;Excuse me, but when is the next train?&quot;, she asked, politely. I told her that the first Purple line train wouldn&#39;t be for another thirty minutes. &quot;Aren&#39;t you feeling cold?&quot;, I asked her, with concern. &quot;A little&quot;, she admitted. She must have been downplaying it to be polite. At this point, I showed her how to enter the station and take a seat in a sheltered area. I then proceeded to offer her my coat, to which she shook her head and said, &quot;It&#39;s very kind of you, young man, but I don&#39;t want you freezing.&quot; &quot;I thought the first train was at 5, but why are you here so early?&quot;, she asked, after a brief pause. I explained that I often didn&#39;t get sleep, and this was routine for me. We then proceeded to make pleasant conversation about family, friends, and life in Massachusetts.
At forty past five, we started seeing other passengers. Nearly all of them were going to work. Time passed quickly, and I was soon on the train. After a quick visit to the washroom, I took a seat beside the woman, and started reading Infinite Jest. It was superbly written, and would take me weeks to finish.
I got off at Natick at forty past six, and bid the woman goodbye. I smiled to myself for another satisfying morning, as I walked towards the office.</summary>
    <title>The Insomniac</title>
    <updated>2021-09-05T09:31:15+02:00</updated>
    <dc:date>2021-09-05T09:31:15+02:00</dc:date>
  </entry>
  <entry>
    <id>art/ml</id>
    <link href="art/ml"/>
    <summary>We investigate the limitations of statistical methods, a subclass of which is called &quot;machine learning&quot;, taking the opportunity to touch on several aspects of the industry, including the future of employment, various industry practices, and privacy. It&#39;s written as a wide-audience article, and only assumes a passing familiarity with software and statistics. We start from a seemingly hardline stance, and follow a line of flawed reasoning leading up to the point where humans become near-obsolete, an event commonly known as the &quot;singularity&quot;. Finally, we step back to see where we stumbled.
We use the following starting point: using any combination of statistical methods, of any complexity, it is impossible to develop any sort of &quot;understanding&quot; that resembles human intelligence. Instead of trying to tackle questions about the nature of intelligence, or entering a deep philosophical rabbit-hole about whether &quot;intelligence is computable&quot;, we start with the modest goal of understanding what statistical methods are.
As a starting point, one might argue that the world is made of up of a collection of &quot;facts&quot; that need to be &quot;memorized&quot; after having seen them stated multiple times correctly. A variation of this is the &quot;monkey brain&quot;, in which you train the monkey by asking it to perform a certain task, rewarding it every time it gets it right, and penalizing it every time it gets it wrong. This is the basic idea behind &quot;reinforcement learning&quot;, the kind used to get machines to play &quot;games&quot;. Speed up the task-to-[reward or penalty] cycle exponentially, and you have a &quot;smart monkey&quot;. While the &quot;smart monkey&quot; might make silly mistakes on a bad day for irrational reasons, a &quot;bad day&quot; for a statistical model is the event when it is presented with data that it doesn&#39;t know how to fit. The monkey&#39;s emotional engine can arguably be emulated by weighting the penalty that the machine receives, based on how fundamental the mistake was, and how many hours of training it had received. The procedure we&#39;ve described of &quot;training&quot; a machine like this falls under the class of &quot;supervised learning&quot;, which, as one might argue, is the same way a human learns from an assignment or exam.
Let us assume that a machine can successfully memorize trillions of &quot;facts&quot;, and look into how these might be represented. In most commonly-used applications today, they are encoded in the form of matrices of floating-point numbers, with some weighted arithmetic operations connecting them, as chosen by the person building the model. The model is fed a matrix of floating-point numbers, and outputs a matrix of floating-point numbers. We might argue that the inability to interpret these individual numbers is akin to not being able to interpret the voltages of the individual neurons in the brain, and this doesn&#39;t seem to be a problem in practice. The said &quot;facts&quot;, now lost in a soup of numbers, can be generalized into a broader class of &quot;things that can be inferred from data&quot;. With every attempt at being able to handle a new situation, the model simply does arithmetic on the soup (a large class of which are just matrix multiplications), and the numbers in the soup adjust themselves with every &quot;feedback&quot; we give it. This is the basic idea behind &quot;neural networks&quot;. While it&#39;s easy to get lost in the details, note that we&#39;re still doing statistical analysis. The additional &quot;frills&quot; haven&#39;t changed the essential nature of the method: the machine is &quot;learning by example&quot;, and attempting to &quot;generalize&quot; it.
Among human beings, learning by example is a very effective way to learn, when starting out. Most programmers would attest to the fact that they started out by copying examples, tweaking them, and checking them with a program (these programs are known as &quot;compilers&quot;). Some would argue that this approach doesn&#39;t scale in human beings, because they don&#39;t have the computational power to continue learning by example, and &quot;cheat&quot; by reading some kind of specification as they become more experienced. Does it mean that classical programmers (aka. those not engaged in building statistical models) could be replaced by machines in the future?
The key to seeing the viability of this argument is to consider two things: (1) the structural complexity of the task, and (2) the number of examples available to concretely reinforce the patterns, and nullify the anti-patterns. It&#39;s no secret that there are several good tools for aiding with &quot;simple&quot; programming languages (otherwise called IDEs), which a reductionist would phrase as &quot;machines are good at dumb tasks&quot;. Using simple frequency analysis, the IDE can order its auto-complete suggestions and corrections intelligently. To date, there is no IDE that can assist with the help of the vast repositories of open-source code, but it doesn&#39;t require a big leap in imagination to assume that this will be possible in the future. In fact, we should be open to the possibility that compilers start building repositories of error-messages, and run in the background, suggesting even more intelligent fixes, based on how others who encountered a similar error fixed it. A philosophically-minded reader would be interested in whether the &quot;intent&quot; of the programmer can be dissected, and if, by labelling various segments of publicly available code with some kind of abstract &quot;function&quot;, the machine can write entire programs with a few prompts. Assuming for a minute that this might be possible, can classical programmers stay relevant by simply moving to newer languages or technologies, on which there are few examples?
This poses a quandary: are we playing some kind of primal game, where the prey constantly has to come up with new strategies to outrun the predator? Some would argue that by refining our statistical methods, one can emulate the process of generalization effectively enough to beat the prey&#39;s creativity on the &quot;search space&quot; completely. What if this is the nature of every human specialization? An event, commonly referred to as the &quot;singularity&quot;, whereby humans become near-obsolete, is predicted in 2040. Why is this claim so ludicrous?
To better understand where statistical models are successful, let us enumerate some recent accomplishments in the field.
Beating the best human player at Go, a program referred to as AlphaGo. The problem is well-defined, and there are very few starting rules. How does a human become good at Go? Just like in Chess, the rules play an insignificant role in the player&#39;s learning, and most of the training is about analyzing previous games. There are huge databases of expertly-played games available, and human players usually try to follow a semi-systematic approach. It could be argued that the machine is inefficient, because it doesn&#39;t follow a systematic approach, but the raw computational power makes this handicap look irrelevant. It&#39;s a little like arguing that a machine doesn&#39;t know the decimal system, or the tables of multiplication, and simply flips 0s and 1s to perform arithmetic. Humans here are the disadvantaged class in both cases, and it is completely unsurprising that, despite certain &quot;inefficiencies&quot;, AlphaGo essentially does what a human player does to beat them at it: analyzing lots of games.
A language engine, referred to as GPT-3. How do humans master a natural language? They do it semi-systematically, starting from the association of simple words to real-world objects, then moving on to phrases describing something, learning some construction rules and absorbing culture along the way; and finally by reading a lot of good literature. Of course, the grammar rules play a very small part of the overall learning, so it is unsurprising that GPT-3 is able to produce grammatically correct responses to human-supplied prompts. However, literature talks about things in the real world, and for it to make sense, sufficient interaction with the real world is a prerequisite. It&#39;s not a bounded game, where all the necessary information is contained on a map or a board. It is, therefore, entirely unsurprising that GPT-3 can&#39;t &quot;know&quot; whether dropping a metallic plate would cause it to melt. It cannot infer the relationship between real-world objects, nor can it differentiate between factual and fictional human experiences. Producing good literature requires a whole lot more than that. This landmark achievement, despite having access to terabytes of text, and an unimaginable amounts of compute power, has failed to infer much more than grammar, sentence construction, and context.
The translation engine behind Google Translate. Someone who has just started using the product with no knowledge of the underlying technology would be puzzled about why this is chalked up as an achievement. Indeed, it&#39;s unlikely that we&#39;ll even have a half-decent translation engine using purely statistical methods. Translation engines have been around for a long time, and their latest iteration might be a significant improvement over the previous ones, but on an absolute scale, something seems to be off about relying on a pure statistical method to learn natural language.
Medical diagnostics. In this field, it is absolutely imperative to organize every little piece of data about every patient in a highly systematic manner: a missing entry is a life-or-death situation. On millions of these remarkably well-organized records, it should come as no surprise that a machine can serve as a valuable aid to a doctor. Statistical models have been a resounding success in this area. The achievement here, perhaps, should be attributed to the subfield of &quot;computer vision&quot;, which studies the best way to encode medical images and videos in the aforementioned number soup.
Proof-search on a well-defined problem phrased in a computer language, using well-defined operations to go from one step of the mathematical proof to the next (in other words, aiding proof-search in programs known as &quot;proof assistants&quot;). These are classic combinatorial problems, with huge search spaces, lots of rules at each node, and clear end goals. Sort of like a video game on steroids. Some progress has been made, and the IDEs are expected to get better. At first glance, this might seem a bit alarming to those who don&#39;t understand how little mathematics can be written down in this form, or how painful it is to do so.
Finding protein-folding structures, referred to as AlphaFold. Another classic combinatorial problem, with well-defined rules. The landmark achievement here is that this problem has an enormous search space, and a solution to an important problem in computational biology has been found, which is sure to accelerate research in the area. Statistical models are an unabashed success here, and it has done what humans could only dream of doing using other methods.
Facial recognition. A classic pattern-recognition problem, which can only be learnt by example. It suffices to say that this technology is a resounding success, with the unfortunate side-effect of opening the door to various abusive ways in which it can be used.
Automated support over voice and text. The little &quot;domain knowledge&quot; needed here is all encapsulated within a very narrow context. There are few simple rules of the type &quot;if the human says this, then say that&quot; (the model used to make such decisions are called &quot;decision trees&quot;), and humans learn to identify voice by example. The landmark achievement here is in &quot;speech synthesis&quot;, or making the machine do text-to-human-like-speech. Despite that, it should be noted that speech-to-text and speech synthesis is still an unsolved problem for non-standard accents. We could argue that there isn&#39;t enough data on the non-standard accents, and yes, that&#39;s what this particular problem boils down to.
More classical problems. Statistical methods have been used to improve upon classical data structures and algorithms in computer science (there was one beating quicksort recently). Analyzing the numbers for patterns, then building a data structure that&#39;s best suited for those patterns, before deciding how to handle them, sounds pretty sensible. The landmark achievement here is that this model can be trained and executed faster than the corresponding classical algorithm.
Now, we&#39;re ready to see machines fall flat on their faces when presented with problems that are effortless for humans.
You&#39;re given a sequence of stones, and each stone is represented by a unique symbol. What can you say about the following sequence?
A kindergartner would simply say that there are more stones on the right. Open-ended problems like this present a formidable challenge to machines. How is it supposed to begin approaching the problem? It is already stuck with the concrete, which it is looking at much too closely.
Two identical twins, jump off from a height of 10ft simultaneously. The first time, they jump without touching each other, and the second time, they hold hands. What is the difference between the two flight timings?
This thought experiment, if not already familiar, should be a simple task for any human to simulate in their heads. It is often used to illustrate the elegance of a simple physical law. Machines have no idea what to do other than to try some kind of correlation on existing data, or if height of 10ft is relevant.
I&#39;ve drawn a certain kind of matrix here. Do you see what I&#39;m trying to show you?
Anyone with minimal exposure to linear algebra would immediately say that it&#39;s a lower-triangular matrix, even if they&#39;ve never seen it presented that way. Yes, it&#39;s vague, but this is a classic abstraction problem, working across domains. The machine, in this case, might identify some kind of right-triangle in the textual picture, but what&#39;s the next step? How does this connect to a rectangle filled with numbers? Where are the numbers?
Notice that very little knowledge is needed to solve the above problems, and collecting more data isn&#39;t going to change a thing.
Solving these seemingly childish problems might not be of much consequence, but they are prerequisites for tackling more complicated problems:
Given that a hedgehog is a rodent that some people keep as pets, which of these six pictures is the picture of a hedgehog?
Derive the equation of motion for a simple pendulum from the axioms of classical mechanics.
Watch a film, and critically comment on the acting, screenplay, direction, and storyline.
Deliver a talk on black holes, and gauge the level of understanding of the audience from the questions you receive.
Check the correctness of the proof of the abc conjecture, by directly reading the document uploaded on arXiv.
To claim that there even is such as thing as a &quot;singularity&quot;, requires, at the very least, some kind of strategy to tackle the last problem.
At this point, the data-zealots would interject. If we collected all the data of every second of every human&#39;s life for a year, given enough storage space and compute power, we&#39;d be able to tackle at least some of these problems using current methods, and our methods are only going to get better over time, they&#39;d argue. Even if it were true, it is a ridiculous claim, akin to something along the lines of &quot;given enough time, I can write a graphical web browser that runs on bare-metal, from scratch, by writing a sequence of 0s and 1s on a piece of paper&quot;. Actually, it&#39;s even more ridiculous than that, because you&#39;d need entire galaxies of low-wage unskilled workers to &quot;clean&quot; and &quot;annotate&quot; the collected data to spoon-feed the machine. Is this the &quot;singularity&quot; you talk about, where humans are doing the equivalent of cleaning ditches to keep the omniscient being happy? If you can&#39;t do kindergartner-level problems today without human assistance, with so much compute power at your disposal, what hope is there of claiming any &quot;intelligence&quot;? But, they&#39;d argue, what if that machine has an intelligence that&#39;s &quot;different&quot;, but somehow &quot;subsumes&quot; human intelligence? This is the kind of pseudoscientific rubbish that&#39;s spewed by nutty cult leaders. Statistical methods have a firm place in modern society, but as it currently stands, machines are the ones with the dunce hats in play-school.
We&#39;re currently in a data-warzone era, where all the big players are racing to collect more data on their users. What kind of data? Boring, inconsequential, everyday lives of human beings. To put it uncharitably, go through everyone&#39;s garbage with computing clusters, and you&#39;re bound to find some half-eaten chocolates. The models behind AlphaGo, GPT-3, and AlphaFold have the same essential nature as the ones used to keep us endlessly hooked onto social platforms, and to sell us unwanted products via sweet-talking voice-assistants. A lot of industrial AI research is enormously expensive, but the dirty secret is that it is paid for with the privacy of decent unassuming folk.
Let strangers sift through your garbage if you please, but don&#39;t do it while being drunk on the &quot;singularity&quot; kool-aid. $\Box$</summary>
    <title>What machines can and can&#39;t do</title>
    <updated>2020-12-01T07:41:57+01:00</updated>
    <dc:date>2020-12-01T07:41:57+01:00</dc:date>
  </entry>
  <entry>
    <id>art/pedestrian</id>
    <link href="art/pedestrian"/>
    <summary>We argue that, underneath all the large-language model hype, there is a pedestrian technology that has the potential to automate a class of problems that classical software can never do.
To study the capabilities of large-language models in the context of writing code, we study two extreme cases. First, the application to a critical project with high cognitive complexity, consisting of 40 million lines of code, and second, the application to a simple from-scratch toy project.
For the first study, I tried incorporating the technology into everyday-contributions to LLVM. The suggestions have over a 95% reject-rate, and the good suggestions appear when you&#39;re editing one instance of a mechanical pattern, and want to change all instances.
For example, consider the following transformation:
to:
This can, in theory, be done by a find-and-replace with a regex, but nobody ever does that for a couple instances.
As another example, consider updating argument names and corresponding comments like:
to:
A find-and-replace would match the &lt;mark&gt;L&lt;/mark&gt; in the &lt;mark&gt;Loop&lt;/mark&gt; argument, erroneously replacing it with &lt;mark&gt;P1&lt;/mark&gt;.
You could say that all the automation boils down to a smart and automatic find-and-replace in this study, but it is important to note this class of problems can never be automated by classical software.
For the second study, I tried applying the technology to writing a tree-sitter based LLVM IR parser. The entire task is a mechanical chore of reading docs and ample examples in the test suite, and encoding the knowledge in the parser. To study the limitations, I tried generating the entire parser using a prompt, but the result was entirely unusable. When writing the parser from a clean-slate, the suggestions were actually quite good. Consider the auto-complete with the following snippet:
to:
which is actually a smarter version of a copy-paste with some knowledge from the internet. Of course the code needs editing, but this is the kind of copy-pasting you&#39;d normally do before making edits.
The technology was also able to generate 300 tests to exercise the parser, and the tests needed little tweaking to pass: this task can be boiled down to copying reduced-case LLVM IR from the test-suite, and matching them with parser nodes. Of course the generated tests aren&#39;t high-quality, but about half the tests are usable as-is, and a lot of tedium has been eliminated.
To conclude, the technology is at alpha-stage, as the automation comes at the cost of putting up with bad visual feedback and fighting the auto-complete nearly all the time, but promises a new class of mechanical automation, whose utility is higher in small codebases following mechanical patterns.</summary>
    <title>The pedestrian technology underneath the hype</title>
    <updated>2025-07-02T14:18:51+01:00</updated>
    <dc:date>2025-07-02T14:18:51+01:00</dc:date>
  </entry>
  <entry>
    <id>art/pl</id>
    <link href="art/pl"/>
    <summary>We illustrate how different programming languages influnced each other, and include a quick FAQ write up.
An FAQ follows.
C++ templates are incredibly powerful, and with the introduction of compile-time expressions, vast portions of modern C++ programs are just compile-time tables. Examples like this make Rust look very tiny:
Rust didn&#39;t restrict what macros can do as much as C++ did. As a result, it&#39;s possible to do more with them. The particular example of the Pest parser is especially enlightening:
Essentially, &quot;grammar.pest&quot; is processed at compile-time, and you can use definitions parsed from it in your code. However, there are trade-offs; Rust&#39;s macros are very slow, and your compile-times blow up if you have recursive macros. &lt;mark&gt;std::embed&lt;/mark&gt; in C++23 will probably do it right.
Haskell is pleasant to get started with, and write relatively simple programs in. In the 2010-2015 period, there was a lot of intellectual discourse and PL research around it. The high-brow crowd was obsessed with transactional memory, parser combinators, and lenses. Online resources were exploding: LYAH and CatProg enjoyed their bouts of popularity. Several people and companies invested in Haskell heavily in that period. The language is easy to get started with, and has a pleasant development experience for relatively simple programs. The problems started when codebases started growing in size and complexity.
You either need to be able to interactively debug your program or prove that it is correct: in Haskell, you can&#39;t do either; the best you can do is to write some QuickCheck tests. Then there&#39;s Liquid Haskell that allows you to pepper your Haskell code with invariants that it will check using Z3. Unfortunately, it is very limited in what it can do: good luck checking your monadic combinator library with LH. Moreover, there are no tools to help you debug the most notorious kind of bug seen in a complicated codebase: memory blowups caused by laziness. It suffices to say that tooling is weak. In Atom, the Haskell add-on was terrible, and even today, in VSCode, the Haskell extension is among the most buggy language plugins.
There are &lt;a href=&quot;https://gitlab.haskell.org/ghc/ghc/-/blob/a1f34d37b47826e86343e368a5c00f1a4b1f2bce/compiler/GHC/Driver/Session.hs#L3729-3876&quot;&gt;over 120&lt;/a&gt; language extensions, which can be turned on/off in each &lt;mark&gt;.hs&lt;/mark&gt; file. The issue is that different extensions interact in subtle ways to produce bugs, and it&#39;s very difficult to tell if a new language extension will play well with the others (it often doesn&#39;t, until all the bugs are squashed, which can take a few years). The best case scenario plays out like this: GHC rejects your program, and suggests that you turn on some other language extensions; you turn them on, and you&#39;re left with a cryptic error message coming from a language extension you&#39;re not as familiar with; you spend the next N hours reading whatever little information is available about the more recent language extension, and decide to throw in the towel. The worst case plays out as follows: the type-checker hangs or crashes, and you&#39;re on the issue tracker searching for the issue; if you&#39;re lucky, you&#39;ll find a bug filed using 50~60% of the language extensions you used in your program, and you&#39;re not sure if it&#39;s the same issue; you file a new issue. In either case, your work has been halted.
There is almost zero documentation on language extensions. Hell, you can&#39;t even find the list of available language extensions with some description on any wiki. Looking at the big picture: first, this is a poor way to do software development; as the number of language extensions increase, your testing burden increases exponentially. Second, the problem of having a good type system is already solved by a simple dependent type theory; you study the core, and every new feature is just a small delta that fits in nicely with the overall model. As opposed to having to read detailed papers on each new language extension. And yes, there&#39;s a good chance that very few people will be able to understand your code if you&#39;re using some esoteric extensions. In summary, language extensions are complicated hacks to compensate for the poverty of Haskell&#39;s type system.
In practice, you&#39;ll be familiar with ~20 language extensions, and use various combinations of them over and over again, so the problem might not seem as acute as a regular Haskell programmer. However, PL research has shifted away from Haskell for the most part, and the little that happens tends to be &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2019/03/ho-haskell-5c8bb4918a4de.pdf&quot;&gt;unnecessarily complex nonsense&lt;/a&gt; that never sees the light of day.
Here&#39;s a sample of some simple Haskell code pieced together. I&#39;ve intentionally left out examples using &lt;mark&gt;LinearTypes&lt;/mark&gt;, because it&#39;s unfair to find faults with such a recent language feature.
What does the code mean? What is the intent?
Can you specifically tell how each language extension was useful in the snippet?
Guess what the code will do on GHC 8.10.1. What will an older version of GHC do?
How many more language features are missing?
It does, but the author didn&#39;t feel that it was worthy of an asterisk, because the compiler is working with a really dumb language. Nevertheless, the GC and scheduler are praise-worthy, as is the overall experience with ultra-low compile-times.</summary>
    <title>An opinionated history of programming languages</title>
    <updated>2020-09-30T12:43:32+02:00</updated>
    <dc:date>2020-09-30T12:43:32+02:00</dc:date>
  </entry>
  <entry>
    <id>art/ra</id>
    <link href="art/ra"/>
    <summary>A picture of artwork.</summary>
    <title>Ra: Egyptian ink on parchment</title>
    <updated>2020-12-27T05:50:10+01:00</updated>
    <dc:date>2020-12-27T05:50:10+01:00</dc:date>
  </entry>
  <entry>
    <id>art/wet-dream</id>
    <link href="art/wet-dream"/>
    <summary>Capitalism works by getting a small number of skilled humans to produce something or offer some service that can then be marketed and sold to a vast consumer class. We live in very exciting times, for corporations have stumbled on a piece of technology that will take the capitalist wet dream to its absolute end: eliminating a vast number of skilled humans from the process, generating vast amounts of low-quality content instantaneously. Yes, generative AI will take jobs away from artists, writers, journalists, doctors, therapists, programmers, musicians, actors, lawyers, and several other professions. Anyone who tells you otherwise is lying. Capitalism has always been a race to the bottom: if your product or service offers something valuable at a competitive price, consumers will give you their money. Amazon and Uber are prime examples of this, and both create a sharp divide between the workers they exploit and the customers who&#39;re paying them. Whatever happens next with generative AI should therefore come as no surprise to anyone familiar with the basics of capitalism.
In this race to the bottom, there will be a vast number of people who can&#39;t afford human labor and are stuck in the free tier of ad-supported AI-generated content. The amount of such content available will explode by five or ten orders of magnitude, leading to greater consumption, and hence higher ad revenues. Most ads in this tier will be similarly AI-generated, as advertisers would also want to cut costs. Of course, corporations will say that the vast majority&#39;s quality of life has improved, as these poor people would now have access to a free AI doctor, therapist, and companion, AI summaries of content they can&#39;t afford, in addition to the vast amounts of low-quality misinformation and entertainment. The number of skilled humans required will shrink, as will the output: for the average person, buying such content will become a luxury. Wealthy people today already live in separate worlds, and they&#39;re already a different class of consumer, so it&#39;s not very hard to imagine what the world will be like once this technology becomes widespread.
Lengthy articles and debates on the internet seriously evaluating generative AI technology amuse me. It&#39;s supposed to be cheap unglamorous rubbish that you peddle to the masses, and I&#39;m sure the people making the calls at corporations are chuckling too.
AI is something you pay to avoid, just like ads. $\Box$</summary>
    <title>The capitalist wet dream</title>
    <updated>2025-01-04T19:18:03+00:00</updated>
    <dc:date>2025-01-04T19:18:03+00:00</dc:date>
  </entry>
  <entry>
    <id>compilers/2025-llvm</id>
    <link href="compilers/2025-llvm"/>
    <summary>As the year draws to a close, it helps to look back and reflect on some key changes in LLVM. The article is written from my perspective, summarizing work I landed, along with adjacent work, in chronological order.
In September 2024, Nikita Popov proposed &lt;a href=&quot;https://discourse.llvm.org/t/rfc-signedness-independent-icmps/81423&quot;&gt;signedness-independent integer-compares&lt;/a&gt;, and the necessary work for enabling optimizations with it occupied the first two months of my 2025. In January, Florian Hahn introduced the initial version of a &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/Analysis/ScalarEvolutionPatternMatch.h&quot;&gt;pattern-matcher for ScalarEvolution&lt;/a&gt;, which I would flesh out and use to improve code over the course of the year.
March and April were not very productive, as I was on vacation and attending EuroLLVM. In the little time I had, I fixed a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/137005&quot;&gt;vectorization bug&lt;/a&gt; and improved code by &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/134373&quot;&gt;introducing a new iterator in a fundamental data structure&lt;/a&gt;.
In May, I introduced a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/125365&quot;&gt;constant folder&lt;/a&gt; in VPlan, guided by Florian, and fixed another &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/136858&quot;&gt;vectorization bug&lt;/a&gt;. In collaboration with Mel Chen, I made a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/118393&quot;&gt;small improvement to the routine that checks patterns for vectorization&lt;/a&gt;.
In June, an &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/Analysis/HashRecognize.cpp&quot;&gt;analysis to recognize cyclic-redundancy-check loops&lt;/a&gt; landed, but required several months of improvements, before finally &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/143208&quot;&gt;enabling optimization of CRC loops&lt;/a&gt; in September: the work was done in close collaboration with Piotr Fusik. While reviewing one of Florian&#39;s patches, I also &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/141752&quot;&gt;extended a pattern for vectorization&lt;/a&gt;, which Florian built upon.
In July, I was occupied with instruction selection and &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/146058&quot;&gt;cost-modeling work&lt;/a&gt; for the intrinsics &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/145898&quot;&gt;lrint&lt;/a&gt;, &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/147713&quot;&gt;lround&lt;/a&gt;, and &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/152476&quot;&gt;ldexp&lt;/a&gt; on RISC-V, guided by Craig Topper.
In August, Luke Lau used parameter packs to &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/152272&quot;&gt;greatly improve the pattern-matcher&lt;/a&gt; in VPlan. Inspired by another of his patches, I would introduce a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/154771&quot;&gt;integer/floating-point compare matcher&lt;/a&gt;, using it for optimization benefit. I would also fix some issued related to wrap-flags in VPlan.
In September, I introduced a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/151872&quot;&gt;common-subexpression-elimination transform&lt;/a&gt; in VPlan, guided by Florian. I also picked up and completed one of Luke&#39;s abandoned patches, a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/159386&quot;&gt;match functor&lt;/a&gt;. Nikita would introduce &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/Transforms/Scalar/DropUnnecessaryAssumes.cpp&quot;&gt;a new transform&lt;/a&gt; to strip unnecessary &lt;mark&gt;llvm.assume&lt;/mark&gt; instructions.
In October, I would obsess over the loop-invariant code motion in VPlan, and &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/162674&quot;&gt;factor out the legality check for hoisting/sinking&lt;/a&gt;, fixing a miscompile. My routine would bail out on memory operations due to aliasing issues, and I was thinking about how to get aliasing information in VPlan.
In November, I would &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/168354&quot;&gt;fix long-standing issues related to wrap-flags&lt;/a&gt; in VPlan, and make an improvement to a routine to undo sub-optimal widening decisions in VPlan. Florian would build upon my improvement to yield the &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/168246&quot;&gt;final version&lt;/a&gt;. He would also create specialized hoisting/sinking routines for memory operations, based on the &lt;mark&gt;noalias&lt;/mark&gt; metadata: I plan to re-use the legality checks to improve the general case.
Finally, we&#39;re in December. The main outstanding patches are &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/168886&quot;&gt;directly unrolling a recipe&lt;/a&gt; in VPlan, and introducing a &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/168731&quot;&gt;carry-less multiply intrinsic&lt;/a&gt;, whose design was guided by Craig and Piotr. Both will hopefully land soon.
Although I wasn&#39;t either directly or indirectly involved, &lt;a href=&quot;https://github.com/llvm/llvm-project/pull/153821&quot;&gt;llvm-lit --update-tests&lt;/a&gt; introduced in August, has completely eliminated the tedium associated with mass test-updates!</summary>
    <title>My 2025 in LLVM: samesign, HashRecognize, and VPlan</title>
    <updated>2025-12-08T18:13:01+00:00</updated>
    <dc:date>2025-12-08T18:13:01+00:00</dc:date>
  </entry>
  <entry>
    <id>compilers/abibug</id>
    <link href="compilers/abibug"/>
    <summary>Our custom max function returns the maximum of two positive integers, and returns the negative integer, given a positive and negative integer. Classic signed wrapping, you&#39;d think. It&#39;s not so simple, as the problem reproduces only under the following circumstances:
LLVM code is calling the max function.
The library containing the max function is compiled without debugging information.
The entire world has been built with XCode 6+.
More information: The LLVM IR is exactly the same between an XCode 5 sandbox, an XCode 6 sandbox, and a GNU/Linux sandbox. The corresponding assembly diff is also clean.
The corresponding C++ code (is actually specialized with short) is:
While in lldb, the assembly instructions look like:
So, it&#39;s comparing the extended registers, but cmpl/cmpw difference is
messing up somehow.
After execution of the cmpl/cmpw:
In conclusion, this is a Clang/LLVM version mismatch. It&#39;s compiling muIntScalarMax_sint16 to emit a compl instead of a compw. The fact that the bug only reproduces with LLVM is because: LLVM doesn&#39;t set up the extended form of the register correctly for negative numbers, when calling a function with a word-sized register.
The SysV ABI does not guarantee that the i16 will be sext&#39;ed to i32. Clang ABI is probably an enhancement over the SysV ABI, and LLVM 3.5 doesn&#39;t know about this.</summary>
    <title>An ABI-mismatch bug</title>
    <updated>2015-08-23T16:25:56-05:00</updated>
    <dc:date>2015-08-23T16:25:56-05:00</dc:date>
  </entry>
  <entry>
    <id>compilers/backend</id>
    <link href="compilers/backend"/>
    <summary>Any modern compiler has three main components: a front-end that parses the source language, and translates it to a middle-end intermediate representation, a middle-end that operates on this IR, performing target-independent optimizations with a bit of target-specific information, and a backend that takes the final optimized middle-end IR and emits target-specific assembly. In this article, we illustrate the operation of a the main components of a compiler&#39;s backend, namely instruction selection, instruction scheduling, and register allocation, using LLVM as the reference compiler. LLVM is especially good for the purposes of illustration, as it has, in addition to a clean human-readable middle-end IR, a relatively clean human-readable backend IR. We use the RISC-V target for the purposes of concrete illustration, chosen once again for its clean design.
Let us start with a simple C program as our guiding example, and have a look at each step of the backend lowering after running the entire middle-end optimization pipeline, all the way up to the final assembly.
First, we run the front-end, and the full middle-end optimization pipeline to obtain the optimized LLVM IR that&#39;s handed off to the backend. Running &lt;mark&gt;clang --target=riscv64 -march=rv64gc -O3 -emit-llvm -S&lt;/mark&gt; emits:
The astute reader will note that we have strategically omitted the vector extension of RISC-V (we use &lt;mark&gt;-march=rv64gc&lt;/mark&gt; instead of &lt;mark&gt;-march=rv64gcv&lt;/mark&gt;) to avoid unnecessarily complicating the exposition due to the vectorization passes in LLVM. Auto-vectorization has already been discussed &lt;a href=&quot;/compilers/intro-vec&quot;&gt;previously&lt;/a&gt;.
The first thing to notice is that the body of the function is cleanly separated into basic blocks: entry, ph, common.ph, inner.loop, inner.loop.exit, common.exit, and exit. Next, notice that the IR is in single-static-assignment (SSA) form, which means is that variables like &lt;mark&gt;%iv&lt;/mark&gt; are only ever defined once. Since this is a loop that requires incrementing the induction variable, a fresh &lt;mark&gt;%iv.next&lt;/mark&gt; variable is created, and &lt;mark&gt;%iv&lt;/mark&gt; is a &lt;mark&gt;phi&lt;/mark&gt; node that selects between the values &lt;mark&gt;0&lt;/mark&gt; when jumping into the &lt;mark&gt;loop&lt;/mark&gt; block from &lt;mark&gt;entry&lt;/mark&gt;, and &lt;mark&gt;%iv.next&lt;/mark&gt; when jumping to the &lt;mark&gt;inner.loop&lt;/mark&gt; block from the back-edge of the inner loop. All the instructions (load, getelementptr, store, or, shl etc.) are target-independent instructions that are used throughout the LLVM middle-end. The target-specific instructions will only be seen after instruction selection.
The first step in the lowering process is instruction selection. We can look at the output of this step by running &lt;mark&gt;llc --print-after-isel&lt;/mark&gt;:
What we see is a relatively straight-forward translation of the middle-end LLVM IR to machine IR (MIR), preserving the basic block structure and SSA-form. The purpose of instruction selection is to select a combination of instructions available on the target that optimally represents the middle-end semantics. Its two primary functions are to replace middle-end SSA variables by virtual registers, and to replace middle-end instructions by target-specific instructions.
Each virtual register has a suffix of either gpr (general-purpose register) or fpr (floating-point register). There are also physical registers in this MIR: &lt;mark&gt;$x0&lt;/mark&gt;, &lt;mark&gt;$x10&lt;/mark&gt;, &lt;mark&gt;$x11&lt;/mark&gt;, &lt;mark&gt;$x12&lt;/mark&gt;, &lt;mark&gt;$frm&lt;/mark&gt;. Per the RISC-V calling convention, the three arguments to the function are in general-purpose registers &lt;mark&gt;$x10&lt;/mark&gt;, &lt;mark&gt;$x11&lt;/mark&gt;, and &lt;mark&gt;$x12&lt;/mark&gt; (pretty-printed as &lt;mark&gt;a0&lt;/mark&gt;, &lt;mark&gt;a1&lt;/mark&gt;, and &lt;mark&gt;a2&lt;/mark&gt;). &lt;mark&gt;$x0&lt;/mark&gt; is a special register on RISC-V that always holds the value &lt;mark&gt;0&lt;/mark&gt;. &lt;mark&gt;$frm&lt;/mark&gt; is the floating-point rounding-mode register. None of these can be virtual registers. A minor point to notice is the annotations on the virtual registers, &lt;mark&gt;killed&lt;/mark&gt; and &lt;mark&gt;implicit&lt;/mark&gt;. The operands of the &lt;mark&gt;FMADD_S&lt;/mark&gt;, &lt;mark&gt;%30:fpr32&lt;/mark&gt; and &lt;mark&gt;%31:fpr32&lt;/mark&gt;, are marked as &lt;mark&gt;killed&lt;/mark&gt;, meaning that this is the final definition/use of those registers (these annotations are preliminary: the final annotations will be determined at a later stage). The register &lt;mark&gt;$frm&lt;/mark&gt; is marked as &lt;mark&gt;implicit&lt;/mark&gt;, meaning that it implicitly uses this register, and that this argument will be absent in the final assembly. The virtual registers must be turned into physical registers by the register allocator before emitting the final assembly.
Middle-end instructions like &lt;mark&gt;@llvm.fmuladd&lt;/mark&gt; have been replaced by target-specific instructions like &lt;mark&gt;FMADD_S&lt;/mark&gt; (pretty-printed as &lt;mark&gt;fmadd.s&lt;/mark&gt;), and several pseudo-instructions like &lt;mark&gt;PseudoBR&lt;/mark&gt;, &lt;mark&gt;PseudoRET&lt;/mark&gt;, and &lt;mark&gt;COPY&lt;/mark&gt; have been inserted in the process. The pseudo-instructions must be lowered to real instructions before emitting the final assembly.
There is some minor additional information in this MIR: the live-ins, or registers that are live at the point of entry, and the branch frequency information: &lt;mark&gt;bb.6&lt;/mark&gt; is going to be the successor basic-block of &lt;mark&gt;bb.6&lt;/mark&gt; 97% of the time.
Although the instruction selection infrastructure, SelectionDAG †, is target-independent, we can see that our MIR is clearly specialized for RISC-V. All the information about the registers available on RISC-V are defined in &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/Target/RISCV/RISCVRegisterInfo.td&quot;&gt;RISCVRegisterInfo.td&lt;/a&gt;. The RISC-V instructions themselves come out of the specification &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/Target/RISCV/RISCVInstrInfo.td&quot;&gt;RISCVInstrInfo.td&lt;/a&gt;, and the target-specific code that specifies how middle-end instructions should be lowered to instructions in this specification via SelectionDAG is &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/Target/RISCV/RISCVISelLowering.cpp&quot;&gt;RISCVISelLowering.cpp&lt;/a&gt;.
In order to understand the various steps after instruction selection, let us &lt;a href=&quot;https://godbolt.org/z/TxzjEMr5M&quot;&gt;inspect the diffs&lt;/a&gt; produced at each step by running &lt;mark&gt;llc -print-changed=diff&lt;/mark&gt;.
The first significant pass to run after the instruction selection is &lt;mark&gt;early-machinelicm&lt;/mark&gt;, which is just a backend version of the middle-end loop invariant code motion transform. Next, we have &lt;mark&gt;livevars&lt;/mark&gt;, which analyzes live variables, and marks registers as &lt;mark&gt;killed&lt;/mark&gt; aggressively.
&lt;mark&gt;phi-node-elimination&lt;/mark&gt; runs next, which removes PHI nodes, leaving the MIR in SSA form by inserting pseudo COPY instructions:
The next pass &lt;mark&gt;liveintervals&lt;/mark&gt; undoes many of the &lt;mark&gt;killed&lt;/mark&gt; annotations inserted by &lt;mark&gt;livevars&lt;/mark&gt;, by performing a more sophisticated analysis to determine the actual virtual registers killed in the loop-nest.
The final preparatory step before instruction scheduling and register allocation is &lt;mark&gt;register-coalescer&lt;/mark&gt;, which eliminates many of the COPY instructions, taking the MIR out of SSA form:
The purpose of the instruction scheduler, &lt;mark&gt;machine-scheduler&lt;/mark&gt;, is to re-order instructions to minimize hazards and stalls. The optimal schedule depends on micro-architectural details, and an example of a scheduler descriptor is &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/Target/RISCV/RISCVSchedSiFive7.td&quot;&gt;RISCVSchedSiFive7.td&lt;/a&gt;, for the SiFive 7 CPU. In our invocation of Clang, we did not specify a particular CPU, so some generic information for RISC-V is used. The optimal schedule can be seen in the following diff:
We&#39;re now ready to run the &lt;mark&gt;greedy&lt;/mark&gt; register allocator †, whose purpose is to assign every virtual register a physical register, while re-using the limited number of physical registers on the machine, and minimizing register spills. The actual replacement is done by &lt;mark&gt;virtregrewriter&lt;/mark&gt;:
Before we emit the assembly, there are two pending steps: first, basic blocks need to be re-ordered, so that the final structure is flat and there are no unnecessary jumps; second, pseudo-instructions like COPY need to be replaced by real instructions. The final MIR after &lt;mark&gt;branch-folder&lt;/mark&gt; and &lt;mark&gt;postrapseduos&lt;/mark&gt; is:
The assembly is produced from the final MIR by AsmPrinter, which doesn&#39;t do much more than pretty-printing registers (&lt;mark&gt;$x10&lt;/mark&gt; is &lt;mark&gt;a0&lt;/mark&gt;, &lt;mark&gt;$x11&lt;/mark&gt; is &lt;mark&gt;a1&lt;/mark&gt;), and instructions (&lt;mark&gt;FMADD_S&lt;/mark&gt; is &lt;mark&gt;fmadd.s&lt;/mark&gt;), while dropping auxiliary information. This output can be checked against the &lt;a href=&quot;https://github.com/riscv/riscv-isa-manual/&quot;&gt;RISC-V ISA manual&lt;/a&gt;.
There is an alternative instruction selection framework called GlobalISel, and alternative register allocation algorithms in LLVM.</summary>
    <title>A tour of the LLVM backend</title>
    <updated>2024-08-04T21:35:05+01:00</updated>
    <dc:date>2024-08-04T21:35:05+01:00</dc:date>
  </entry>
  <entry>
    <id>compilers/incrd</id>
    <link href="compilers/incrd"/>
    <summary>An input program function is represented using a Control Flow Graph (CFG). It&#39;s composed of different Statements, which decompose into Expressions, but we will not concern ourselves with Expressions. The main control flow constructs are SplitStatement and MergeStatement. The SplitStatement has outgoing edges to N (N = 2 in the case of IfStatement) other Statements, and the MergeStatement has incoming edges from M other Statements.
The Dominator is a data structure that holds &quot;domination relationship&quot; between any two Statements in the CFG. &quot;A dominates B&quot; means: all paths starting from the start of the CFG to end of the CFG, that flow through A, must necessarily flow through B. It&#39;s represented using a tree of things that progressively dominate their children. We want to incrementally update this structure.
Motivation
Any code motion transform will invalidate the Dominator at each step, and running Cooper or Lengauer-Tarjan on each invalidation is expensive even if the algorithm is near-linear. The number of queries is several times the number of reconstructions; we cannot trade off query-efficiency for ease-of-update.
Background
The main supporting data structure we will use for this feat is the Program Structure Tree, which is incrementally updated. The PST is a tree representation of structured control flow: all Control Flow Graph nodes belong to a Region in a nesting of single-entry single-exit Regions. Any control-flow that cannot be classified is boxed in an UnstructuredRegion, which we will not handle.
This is a Region:
This is a ChainRegion comprising of an IfRegion and a BasicBlockRegion:
This is an UnstructuredRegion, but it is very much SESE:
Updating a vanilla Dominator tree is relatively easy: simply add a child to the correct node. The problem at hand is that we have an O(1)-query Dominator object that we want to keep up to date as we make edits to the IR. How is it O(1)-query? Every node in the tree is assigned DFS start and end times: so if one interval nests within another, the corresponding node is dominated by the other node.
Supporting arbitrary IR edits is both very hard and not a useful goal: we can reduce all our edits to two items, namely inserting a SESE Statement, and inserting a SESE PST Region. Removing a Statement or Region is not interesting, because it just leaves gaps in the numbering, and doesn&#39;t affect any existing Dominance relationships if we make no changes after removal. Modifying a Region can be easily expressed as the removal of the old Region/insertion of the new Region.
The PST gives us an excellent mechanism to classify the blob we&#39;re injecting into the CFG. The interesting Regions begin with a SplitStatement, and end with a MergeStatement, or begin with a MergeStatement and end with a SpiltStatement: IfRegions, SwitchRegions, and LoopRegions. This is the key property we will use to do the incremental update.
Possible approaches
First, it must be noted that by using tight consecutive DFS start and end timers, we will have no choice but to renumber the descendant tree when something is inserted. So, we leave gaps of 64 between the numbering, to stuff our new SESE Statement or Region into. It may be argued that using fractions is profitable, but we found that space to fit intervals wasn&#39;t the bottleneck: the hard problem is determining dominance. When attempting to stuff a new Region into an existing gap, we can imagine centering around the midpoint of the gap, and leaving space for more Regions above and below. Bisecting each gap will exhaust the gaps quickly, and we decided to optimize for insertion only from top to bottom.
One approach we could take is to assume that the Region-to-be-inserted has been removed from a different part of the tree, and has some numbers already that we simply need to adjust them (those numbers will be spaced with gaps of 64, and we need to compress them). This approach wouldn&#39;t be able to handle newly created Regions.
Our algorithm
The key insight in our algorithm is that we need to match a Split Statement with its Merge Statement(s), and need to partition our DFS start/end time intervals into two categories: intervals to trust (ie. ones that exist, and should not be modified), and intervals to assign. The DFS start time corresponds to DFS first-visit sequence, and end time corresponds to DFS final-visit sequence. &lt;mark&gt;interval1.start_time &amp;gt; interval2.start_time&lt;/mark&gt; and &lt;mark&gt;interval1.end_time &amp;lt; interval2.end_time&lt;/mark&gt; means that interval1 is nested within interval2, or that the node corresponding to interval2 dominates the node corresponding to interval1.
The reason we need to match a Split to a Merge is that there are two cases:
The Merge dominates the Split, in which case, the Merge dominates everything lying on the outEdges of the Split leading to the Merge.
The Split dominates the Merge, in which case, the Split dominates everything on its outEdges leading to the Merge.
The intervals of the each of the outEdges (&quot;branches&quot;) must stagger with each other so there&#39;s no false dominance relationship between them. Two intervals &quot;staggering&quot; means one not being nested within the other. In the case of Split-dominates-Merge, they must additionally stagger with the Merge.
Our API looks like:
First, we assign intervals to the split and merge Statements in the Region, by looking at their predecessor and successor. The predecessor is something we need to nest within. Since we decided that we&#39;ll only optimize for the case of top-to-bottom insertion, we can simply add one to the start time, and subtract one from the end time to get the necessary start and end times for the Region entry. For the Region exit, it&#39;s a little more tricky: we need to leave gaps on either side, because there may be nodes along the inEdge(s) as well as nodes along the outEdge(s). For simplicity, let&#39;s simply assign an interval halfway between the Region entry and successor intervals.
Then, we assign intervals to be &lt;mark&gt;branchHeads&lt;/mark&gt; (ie. successors of the SplitStatement whose intervals we need to assign), by staggering with the &lt;mark&gt;headsToStaggerWith&lt;/mark&gt; (ie. successors of the SplitStmt whose intervals we must trust + the MergeStatement if Split-dominates-Merge). &lt;mark&gt;nestWithin&lt;/mark&gt; is the SplitStatement, and &lt;mark&gt;toEnclose&lt;/mark&gt; is &lt;mark&gt;nullptr&lt;/mark&gt;.
Finally, we trivially assign intervals to the rest of the Stmts in the Region, following the Split, by nesting them within the branch heads.
Now, this API generalizes to SESE Statements as well. &lt;mark&gt;branchHeads&lt;/mark&gt; is the singular SESE Statement, &lt;mark&gt;headsToStaggerWith&lt;/mark&gt; may or may not include the containing Region&#39;s MergeStatement. &lt;mark&gt;headsToStaggerWith&lt;/mark&gt; will contain all the other successors of the &lt;mark&gt;SplitStatement&lt;/mark&gt;, if the SESE Statement is one of the successors, and may contain the MergeStatement. If the successor has one inEdge, it&#39;s used as &lt;mark&gt;toEnclose&lt;/mark&gt;.
The actual interval calculation is quite simple. Subtract the &lt;mark&gt;staggerUnion&lt;/mark&gt; interval (union of all the stagger intervals) from the &lt;mark&gt;nestWithin&lt;/mark&gt; interval: this produces two &quot;gaps&quot; which we need to &quot;dice up&quot; into N intervals (where N is the number of &lt;mark&gt;branchHeads&lt;/mark&gt;). The number of intervals in the left gap versus the right gap is dependent on the ratio of the sizes of the gaps. &lt;mark&gt;toEnclose&lt;/mark&gt; simply tells us whether to pick the left gap or right gap in the case of a single &lt;mark&gt;branchHead&lt;/mark&gt;. It&#39;s also useful to check interval exhaustion cases.
There is an elegant way to catch all the cases when the interval has been exhausted. Just &lt;mark&gt;throw&lt;/mark&gt; an exception from the interval-construction class. Catch this exception and abort the incremental update: recompute the Dominator.
Closing notes
Conceptually, the algorithm is simple. The implementation, as leveraged by one transform, passes our entire testsuite (~6k tests).
Our implementation of the Dominator incremental update relies on a correct implementation of the PST incremental update: incorrect information from one incremental update will propagate to the other incremental update and wreak havoc.
The best way to test the implementation is to have a recomputed Dominator as reference, and checking that the dominates-relationship between every two nodes is the same in the incremental update and re-computation case.
Thanks to &lt;a href=&quot;http://playingwithpointers.com&quot;&gt;Sanjoy&lt;/a&gt; for reviewing the draft.</summary>
    <title>Incrementally updating the Dominator</title>
    <updated>2016-10-09T16:28:58-05:00</updated>
    <dc:date>2016-10-09T16:28:58-05:00</dc:date>
  </entry>
  <entry>
    <id>compilers/intro-vec</id>
    <link href="compilers/intro-vec"/>
    <summary>Most modern general purpose CPUs have a vector processing unit (VPU). This unit contains vector registers that can hold multiple integers or floats, and instructions that operate on vector registers. Given two vector registers containing N floats each †, the VPU can perform a single instruction to operate on these two vector registers and store the result in another vector register. Without a VPU, this operation would require N instructions operating on two floating-point registers each relying on the floating-point unit (FPU), or arithmetic logic unit (ALU) in the case of integers.
Major instruction set architectures have vector extensions, and corresponding instructions for the VPU. Examples include x86&#39;s SSE/AVX, AArch64&#39;s SVE/Neon, and the latest entrant is RISC-V&#39;s RVV. In order to generate vector instructions, the programmer could adapt their code to use standardized vector intrinsics. To illustrate, suppose the programmer had the following source code:
If they wanted to use &lt;a href=&quot;https://github.com/riscv-non-isa/rvv-intrinsic-doc&quot;&gt;RISC-V vector intrinsics&lt;/a&gt; to vectorize it, this is how they would adapt the code:
As is evident, this process of hand-vectorizing code is very difficult and error-prone for non-trivial programs. The process is analogous to writing assembly instead of writing the source language and letting the compiler generate assembly. Indeed, compilers today auto-vectorize code, and for the purposes of illustration, let us use Clang/LLVM. First, the source language is lowered to target-independent LLVM IR by Clang, which is the frontend. Next, the vectorizers in the middle-end of LLVM operate on this LLVM IR without vectors in them, and produce LLVM IR with vectors in them. The vectorized LLVM IR is finally lowered to target-specific assembly by the backend of LLVM.
Let us take the same example of &lt;mark&gt;saxpy_golden&lt;/mark&gt;, and see how the loop looks in LLVM IR without vectorization:
The first line, &lt;mark&gt;loop:&lt;/mark&gt;, is like a label in C, to which branch instructions (&lt;mark&gt;br&lt;/mark&gt;) can jump to. The next line is creating a new variable &lt;mark&gt;%iv&lt;/mark&gt; using a &lt;mark&gt;phi&lt;/mark&gt; to select either &lt;mark&gt;0&lt;/mark&gt; when jumping from the &lt;mark&gt;entry&lt;/mark&gt; block (not shown), or &lt;mark&gt;%iv.next&lt;/mark&gt; when jumping from the back-edge of the loop: this is the induction variable of the loop. Since &lt;mark&gt;x&lt;/mark&gt; and &lt;mark&gt;y&lt;/mark&gt; are pointers (marked by the type &lt;mark&gt;ptr&lt;/mark&gt;), we see &lt;mark&gt;getelementptr&lt;/mark&gt; instructions that act as the address computations: so, &lt;mark&gt;%gep.x&lt;/mark&gt; is &lt;mark&gt;%x + %iv&lt;/mark&gt;, and &lt;mark&gt;%gep.y&lt;/mark&gt; is &lt;mark&gt;%y + %iv&lt;/mark&gt;. The &lt;mark&gt;load&lt;/mark&gt; instructions derefence the pointer: so, &lt;mark&gt;%load.x&lt;/mark&gt; is &lt;mark&gt;x[iv]&lt;/mark&gt; and &lt;mark&gt;load.y&lt;/mark&gt; is &lt;mark&gt;y[iv]&lt;/mark&gt;; both are of type &lt;mark&gt;f32&lt;/mark&gt;, or &lt;mark&gt;float&lt;/mark&gt;. &lt;mark&gt;@llvm.fmuladd.f32&lt;/mark&gt; is the computation, which does a &lt;mark&gt;a * x[iv] + y[iv]&lt;/mark&gt;, and the &lt;mark&gt;store&lt;/mark&gt; writes the result &lt;mark&gt;%fmuladd&lt;/mark&gt; back into &lt;mark&gt;%gep.y&lt;/mark&gt;. The last three instructions make the &lt;mark&gt;loop&lt;/mark&gt; block a loop: &lt;mark&gt;%iv.next = %iv + 1&lt;/mark&gt; is the &quot;next value&quot; of the loop induction variable, and &lt;mark&gt;%exitcond&lt;/mark&gt; is a boolean (marked by the type &lt;mark&gt;i1&lt;/mark&gt;, integer with bitwidth 1) that acts as the exit-condition of the loop. &lt;mark&gt;%exitcond&lt;/mark&gt; is computed as an integer-equal-comparison between &lt;mark&gt;%iv.next&lt;/mark&gt; and &lt;mark&gt;%n&lt;/mark&gt;, where &lt;mark&gt;n&lt;/mark&gt; is the total number of iterations of the loop. Finally, there is a branch instruction that jumps to either the &lt;mark&gt;exit&lt;/mark&gt; block (not shown), or back to the &lt;mark&gt;loop&lt;/mark&gt; block (this is the back-edge of the loop), depending on the exit condtion.
This is how it changes after vectorization:
The loop now has a &lt;mark&gt;loop.preheader&lt;/mark&gt; block containing an &lt;mark&gt;%n.and.minus4&lt;/mark&gt; to handle the case when &lt;mark&gt;n&lt;/mark&gt; is not a multiple of four. It also contains an &lt;mark&gt;insertelement&lt;/mark&gt; and &lt;mark&gt;shufflevector&lt;/mark&gt;, which essentially fill the vector &lt;mark&gt;%a.vec&lt;/mark&gt; (that has type &lt;mark&gt;&amp;lt;4 x float&amp;gt;&lt;/mark&gt;) with four copies of &lt;mark&gt;%a&lt;/mark&gt;. The difference in the loop is that &lt;mark&gt;%iv.next&lt;/mark&gt; is now &lt;mark&gt;%iv + 4&lt;/mark&gt;, since it operates on four floats at a time, cutting the number of iterations by a factor of four. The &lt;mark&gt;load&lt;/mark&gt; instructions are now loading four floats at a time, and the vector variant of &lt;mark&gt;@llvm.fmuladd.f32&lt;/mark&gt;, which is &lt;mark&gt;@llvm.fmuladd.v4f32&lt;/mark&gt;, does a &lt;mark&gt;%a.vec * %load.x + %load.y&lt;/mark&gt;, operating on a vector of four floats at a time.
The vectorizer in LLVM is quite sophisticated, and it is fascinating how it manages to vectorize non-trivial examples. To walk through a simple but non-trivial example, consider:
Inspecting the &lt;a href=&quot;https://godbolt.org/z/qq39c1PT7&quot;&gt;output&lt;/a&gt;, we can see what the vectorizer has done. First, we see a new &lt;mark&gt;phi&lt;/mark&gt;:
It has created a new vector induction variable vector, corresponding to &lt;mark&gt;i&lt;/mark&gt;, with four elements, since it has decided to vectorize with a factor of 4. Since &lt;mark&gt;%vec.ind&lt;/mark&gt; is an integer, and the vectors are &lt;mark&gt;float&lt;/mark&gt;, it has done a integer-to-floating-point conversion:
For the conditional itself, it has used a &lt;mark&gt;select&lt;/mark&gt; to choose either &lt;mark&gt;0&lt;/mark&gt; or the loaded value:
For the rest of this article, let us inspect some factors that could get in the way when the loop is expected to be vectorized, omitting cases where the loop is either obviously impossible or unprofitable to vectorize.
The iteration count. In most real-world code, the exact iteration count is unknown and the auto-vectorizers can deal with this. Auto-vectorizers can also deal with non-power-of-2 iteration counts by inserting a scalar epilogue with the remaining iterations. However, if the iteration count of the loop is computable and too small, the loop might get unrolled instead of vectorized.
Aliasing information. If there isn&#39;t sufficient information to determine whether memory accesses in the loop are aliased to each other, runtime checks may be generated, or auto-vectorization might fail completely because insertion of runtime checks make it unprofitable to vectorize.
To illustrate, let&#39;s modify &lt;mark&gt;saxpy_golden&lt;/mark&gt; as follows:
Then, running &lt;mark&gt;clang -O3 -fno-unroll-loops -mllvm -vectorize-loops=false -emit-llvm -S test.c -o test.ll&lt;/mark&gt; followed by &lt;mark&gt;opt -passes=loop-vectorize -debug-only=loop-vectorize test.ll&lt;/mark&gt; &lt;a href=&quot;https://godbolt.org/z/8jn95qbxT&quot;&gt;outputs&lt;/a&gt;:
The overhead it is talking about is runtime checks it needs to add. Adding &lt;mark&gt;restrict&lt;/mark&gt; to the pointers would remove the overhead, and allow it to be vectorized:
Even if we were to ramp up the iteration count to &lt;mark&gt;17&lt;/mark&gt;, unrolling the loop would be preferred over vectorization. Forcing it not to unroll loops using &lt;mark&gt;-fno-unroll-loops&lt;/mark&gt; as before results in successful vectorization. The final &lt;a href=&quot;https://godbolt.org/z/3Torh4ddE&quot;&gt;output&lt;/a&gt; contains a scalar version of the loop as well, marked by &lt;mark&gt;scalar.ph&lt;/mark&gt;, which is required as 17 isn&#39;t divisible by 4.
The memory access pattern. A good rule of thumb write loops with uniform row-major access, and avoid complex indexing arithmetic.
The vectorizer has no issues with the following case:
Now, let&#39;s change the assignment statement in the loop to:
This is unfortunately not vectorized by LLVM, but there is no reason this is not theoretically vectorizable. The reason for this is that an analysis that is used to prove legality of vectorization in LLVM, called LoopAccessAnalysis, is unable to find the bounds of the memory access. Running &lt;mark&gt;opt -passes=loop-vectorize test.ll -disable-output -debug-only=loop-accesses&lt;/mark&gt; &lt;a href=&quot;https://godbolt.org/z/5T9docTfq&quot;&gt;outputs&lt;/a&gt;:
Conditionals within the loop. Certain conditionals are possible to vectorize by either using &lt;mark&gt;select&lt;/mark&gt;, or masking values in the vector using the condition. There is no way to tell if a certain conditional will be vectorized in advance, and depends on whether the pattern is implemented in LLVM&#39;s vectorizer.
LLVM is able to vectorize the example discussed previously:
Unfortunately, it is unable to vectorize this:
The &lt;a href=&quot;https://godbolt.org/z/Wfjr6ofrf&quot;&gt;reason&lt;/a&gt; is as good as &quot;it is unimplemented&quot;:
Whether the loop is canonical. If, for instance, the loop has multiple exits, LLVM cannot auto-vectorize it.
LLVM struggles with early-exit loops, although there is no reason this is not possible to vectorize:
The &lt;a href=&quot;https://godbolt.org/z/Yfe3PxzPY&quot;&gt;reason&lt;/a&gt; is as good as &quot;it is unimplemented&quot;:
Function calls within the loop. Except for calls to LLVM intrinsics (like the &lt;mark&gt;@llvm.fmuladd&lt;/mark&gt; in the example discussed previously), the auto-vectorizer will fail if there are function calls within the loop that it didn&#39;t inline.
LLVM cannot vectorize:
AArch64 and RISC-V don&#39;t include a vector instruction for &lt;mark&gt;printf&lt;/mark&gt; in their instruction set, and this is a fundamental limitation of the instruction set.
An example of a vector instruction that&#39;s included in both AArch64 and RISC-V is &lt;mark&gt;lrint&lt;/mark&gt;, and vectorizing a program with it will use the vector variant of the &lt;mark&gt;@llvm.lrint&lt;/mark&gt; intrinsic in the target-independent LLVM IR. The output can be inspected on &lt;a href=&quot;https://godbolt.org/z/foG7GsxYh&quot;&gt;Godbolt&lt;/a&gt;.
N is a power of two, bounded by vector register width.</summary>
    <title>An introduction to auto-vectorization with LLVM</title>
    <updated>2024-06-27T20:08:57+01:00</updated>
    <dc:date>2024-06-27T20:08:57+01:00</dc:date>
  </entry>
  <entry>
    <id>compilers/loops</id>
    <link href="compilers/loops"/>
    <summary>Detecting loops in a directed graph can be tricky, depending on how you define your loop. If you only want to admit &quot;natural loops&quot;, where the header of the loop dominates every node in the body, as well as the footer, we have a simple algorithm. If you want to go to the other extreme, and define the most general &quot;strongly connected components&quot;, without regard for loops, you have Tarjan&#39;s SCC algorithm. However, I&#39;m going to discuss a definition of loop that admits strange loops, but only things we&#39;d intuitively call &quot;loops&quot;.
There are five [†] reduced cases we must consider, where backedges are marked by dotted arrows:
Now, let&#39;s employ a simple DFS:
We get the following start times and end times:
Let&#39;s call the first number &quot;DFS number&quot;, and the second number &quot;topo number&quot;, to indicate that this is the order that topological sort would have produced, in the absence of backedges (topological sort is meaningless when there are loops).
First pass: do the dfs, and number everything.
Second pass: find the backedge. When &lt;mark&gt;start_time[destination] &amp;lt; start_time[source]&lt;/mark&gt; and &lt;mark&gt;end_time[destination] &amp;gt; end_time[source]&lt;/mark&gt;, we have a backedge from source to destination.
Third pass: find all the nodes in the loop. Here, we walk backwards from the source of the backedge until the loop header, and take everything that&#39;s &quot;nested within&quot; the loop header start and end times to be within the loop.
In the no loop case, there&#39;s nothing to do since no backedges were detected. Note that in the &lt;mark&gt;6/7&lt;/mark&gt;-&lt;mark&gt;2/5&lt;/mark&gt; and &lt;mark&gt;6/7&lt;/mark&gt;-&lt;mark&gt;3/4&lt;/mark&gt; combinations, both start times and end times are greater in one pair; this is how we identify crossedges.
In the early exit case, &lt;mark&gt;2/7&lt;/mark&gt; and &lt;mark&gt;4/5&lt;/mark&gt; are nested within &lt;mark&gt;1/8&lt;/mark&gt;, and we never reach &lt;mark&gt;exit target&lt;/mark&gt; by walking backward from the source of the backedge.
In the multi-entry case, we correctly detect that &lt;mark&gt;1/8&lt;/mark&gt; is not nested within &lt;mark&gt;2/7&lt;/mark&gt;, while &lt;mark&gt;3/6&lt;/mark&gt; and &lt;mark&gt;4/5&lt;/mark&gt; are: when a node reached via a backward walk doesn&#39;t nest within the header, the loop is classified as &lt;em&gt;irreducible&lt;/em&gt;.
In the cuddled loops case, everything nests within &lt;mark&gt;1/8&lt;/mark&gt;, and the inner loop consists of &lt;mark&gt;4/5&lt;/mark&gt;, &lt;mark&gt;3/6&lt;/mark&gt;, and &lt;mark&gt;2/7&lt;/mark&gt;, all of which nest within &lt;mark&gt;2/7&lt;/mark&gt;. Finally, the shared header case is detected as two nested loops as well: a loop is identified by a unique backedge, not a unique header [‡]. The analysis is weak in that these cases are indistinguishable from normal nested loops.
Hat tip to Sanjoy for pointing out the fifth case.
You might want to merge loops that share a header in a post-pass.</summary>
    <title>Detecting loops</title>
    <updated>2016-07-16T16:26:50-05:00</updated>
    <dc:date>2016-07-16T16:26:50-05:00</dc:date>
  </entry>
  <entry>
    <id>compilers/regalloc</id>
    <link href="compilers/regalloc"/>
    <summary>We are going to be discussing LLVM&#39;s Fast Register Allocator: you might like to open &lt;a href=&quot;https://github.com/llvm/llvm-project/blob/main/llvm/lib/CodeGen/RegAllocFast.cpp&quot;&gt;RegAllocFast.cpp&lt;/a&gt; and refer to it as we go through the article.
FastRegAlloc allocates linearly, going through instructions and their operands in order. It uses &lt;mark&gt;PhysRegState&lt;/mark&gt; to keep track of the state of various physical registers: they can be 0 (&lt;mark&gt;regDisabled&lt;/mark&gt;), 1 (&lt;mark&gt;regFree&lt;/mark&gt;), 2 (&lt;mark&gt;regReserved&lt;/mark&gt;), or a virtual register number (a large number). At the time of allocation, the full UseDef information is available (so you have information like &lt;mark&gt;LR.LastUse&lt;/mark&gt;); a register can either be a &lt;mark&gt;&amp;lt;def&amp;gt;&lt;/mark&gt; or a use. If &lt;mark&gt;IsImplicit&lt;/mark&gt; is flipped, it could also be a implicit-def (&lt;mark&gt;&amp;lt;imp-def&amp;gt;&lt;/mark&gt;) or implicit-use (&lt;mark&gt;&amp;lt;imp-use&amp;gt;&lt;/mark&gt;).
turns into
after allocation. The &lt;mark&gt;&amp;lt;kill&amp;gt;&lt;/mark&gt; indicates that the instruction is the last use of &lt;mark&gt;%RDX&lt;/mark&gt;. In another example,
turns into
Notice that the instruction references physical registers even before the allocation. That&#39;s because this instruction specifically wants to sign extend &lt;mark&gt;%RAX&lt;/mark&gt; into &lt;mark&gt;%RDX&lt;/mark&gt; (not any other register) for use in a later &lt;mark&gt;IDIV&lt;/mark&gt;. Remember that &lt;mark&gt;IDIV&lt;/mark&gt; operates on &lt;mark&gt;%RDX:%RAX&lt;/mark&gt; as the numerator, and writes the quotient in &lt;mark&gt;%RAX&lt;/mark&gt;, reminder in &lt;mark&gt;%RDX&lt;/mark&gt;.
The instruction is modeled as &lt;mark&gt;MachineInstr&lt;/mark&gt;, and the operands as &lt;mark&gt;MachineOperand&lt;/mark&gt;. Note that a &lt;mark&gt;MO&lt;/mark&gt; doesn&#39;t have to be a register: it could also be an immediate value; we use &lt;mark&gt;MO.isReg()&lt;/mark&gt; to find out if it&#39;s a register, physical or virtual. In addition to the states shown in the pretty-print, &lt;mark&gt;MO&lt;/mark&gt; can also be &lt;mark&gt;EarlyClobber&lt;/mark&gt;, &lt;mark&gt;Dead&lt;/mark&gt;, or &lt;mark&gt;Undef&lt;/mark&gt;. A Dead &lt;mark&gt;MO&lt;/mark&gt; implies Def, and indicates that the value defined is used no longer.
&lt;mark&gt;AllocateBasicBlock&lt;/mark&gt;, which operates on &lt;mark&gt;MachineBasicBlock&lt;/mark&gt;, can be separated into three different scans, that operate on register MOs. Before the first scan, we set up the &lt;mark&gt;LiveIns&lt;/mark&gt; (registers coming in live from the previous &lt;mark&gt;MBB&lt;/mark&gt;) as &lt;mark&gt;regReserved&lt;/mark&gt; so they aren&#39;t clobbered. The first scan operates on physical registers that are allocatable Uses; it calls &lt;mark&gt;usePhysReg&lt;/mark&gt; on them. At this stage, the physical register must be either &lt;mark&gt;regDisabled&lt;/mark&gt;, &lt;mark&gt;regReserved&lt;/mark&gt;, or &lt;mark&gt;regFree&lt;/mark&gt;. It cannot be allocated to a virtual register. Why? Imagine you see:
Now, if &lt;mark&gt;%RAX&lt;/mark&gt; were already allocated to a virtual register, we&#39;re basically screwed in this linear walk. Otherwise, kill it, mark it as &lt;mark&gt;regFree&lt;/mark&gt;, and move on: we have done our part by completing the use of the register that was reserved earlier.
The second scan operates only on virtual register Uses. We add the register to &lt;mark&gt;LiveVirtRegs&lt;/mark&gt;, via &lt;mark&gt;reloadVirtReg&lt;/mark&gt;. If it didn&#39;t already exist in the map, we &lt;mark&gt;allocVirtReg&lt;/mark&gt; it, which essentially gets the first &lt;mark&gt;regFree&lt;/mark&gt; register not used in an instruction, and calls &lt;mark&gt;assignVirtToPhysReg&lt;/mark&gt; on it. &lt;mark&gt;assignVirtToPhysReg&lt;/mark&gt; is very simple: it just updates the &lt;mark&gt;PhysRegState&lt;/mark&gt; mapping. Finally,
&lt;mark&gt;reloadVirtReg&lt;/mark&gt; updates the &lt;mark&gt;UsedInInstr&lt;/mark&gt; map.
The third scan operates on physical and virtual registers that are Defs. If the register is a physical register, it does &lt;mark&gt;definePhysReg&lt;/mark&gt; with &lt;mark&gt;regReserved&lt;/mark&gt; unless &lt;mark&gt;MO.isDead()&lt;/mark&gt;, in which case it&#39;s regFree. &lt;mark&gt;definePhysReg&lt;/mark&gt; is very simple: it just puts the register in the state that was requested (&lt;mark&gt;regReserved&lt;/mark&gt; or &lt;mark&gt;regFree&lt;/mark&gt;, in this case).
To think about the problem, if we have,
we should always &lt;mark&gt;regReserve&lt;/mark&gt; &lt;mark&gt;%RAX&lt;/mark&gt;, right? Except if &lt;mark&gt;%RAX&lt;/mark&gt; is dead. What about if it&#39;s an &lt;mark&gt;&amp;lt;imp-def&amp;gt;&lt;/mark&gt;?
We didn&#39;t pass &lt;mark&gt;CQO&lt;/mark&gt; &lt;mark&gt;%RAX&lt;/mark&gt; explicitly, but that doesn&#39;t mean that a later instruction (for instance, &lt;mark&gt;IDIV&lt;/mark&gt;) is not relying on this register&#39;s value. If we have an instruction between &lt;mark&gt;CQO&lt;/mark&gt; and &lt;mark&gt;IDIV&lt;/mark&gt;, that can potentially clobber &lt;mark&gt;%RAX&lt;/mark&gt;, leading to a regalloc crash. Hence, we &lt;mark&gt;regReserve&lt;/mark&gt; even if &lt;mark&gt;MO.isImplicit()&lt;/mark&gt;.
The third scan does &lt;mark&gt;defineVirtReg&lt;/mark&gt; on virtual registers, to grab the first free physical register for the virtual register.
Note: the &quot;pretty-prints&quot; are generated by setting a breakpoint in &lt;mark&gt;AllocateBasicBlock&lt;/mark&gt; and executing &lt;mark&gt;p MBB-&amp;gt;dump()&lt;/mark&gt;.</summary>
    <title>Inside a register allocator</title>
    <updated>2016-03-24T17:36:35-05:00</updated>
    <dc:date>2016-03-24T17:36:35-05:00</dc:date>
  </entry>
  <entry>
    <id>compilers/vplan</id>
    <link href="compilers/vplan"/>
    <summary>In a follow-up to the &lt;a href=&quot;/compilers/intro-vec&quot;&gt;introductory article on auto-vectorization&lt;/a&gt;, we discuss how the loop vectorizer works internally. Nearly all transforms in LLVM work by directly manipulating IR, using the results of various analyses on the IR, while the loop vectorizer operates on a layer of abstraction above the IR: it lifts the LLVM IR to an overlay IR, analyzes and transforms the overlay IR, and finally removes the original IR entirely, replacing it with a final LLVM IR from the direct concretization of the final overlay IR. It also plans various vectorization strategies, costing each one until it finds a winning strategy. This infrastructure is called VPlan.
Let us start with a variant of the example program from the previous article:
As the vectorizer runs near the end of the pipeline, LLVM applies various scalar transforms to yield target-independent IR that the vectorizer gets as input:
The planning stage of the vectorizer normally decides two key factors when attempting to find the winning plan: the vectorization factor (VF), which determines the width of the instructions on the VPU, and interleave count (UF, referring to the old name &quot;unrolling factor&quot;), which determines how many copies of each instruction to create. These decisions are predicated on the cost of the final instructions, which only makes sense when a specific target is provided: for the purposes of this article, let us force the vectorizer to run in a target-independent fashion without interleaving using &lt;mark&gt;-force-vector-width=4 -force-vector-interleave=1&lt;/mark&gt;, and inspect the lifting of the LLVM IR to the VPlan:
First, we notice that each variable has an &lt;mark&gt;ir&amp;lt;&amp;gt;&lt;/mark&gt; or &lt;mark&gt;vp&amp;lt;&amp;gt;&lt;/mark&gt; annotation. The &lt;mark&gt;ir&lt;/mark&gt; variables are variables present in the original LLVM IR or some kind of constants, and the &lt;mark&gt;vp&lt;/mark&gt; variables are fresh variables that are created in the VPlan. Next, we notice new annotations like &lt;mark&gt;EMIT&lt;/mark&gt;, &lt;mark&gt;CLONE&lt;/mark&gt;, and &lt;mark&gt;WIDEN&lt;/mark&gt; in each line in the vector loop: these are termed &quot;recipes&quot;, from which the final LLVM IR will be emitted, post transformations.
Near the beginning of the vector loop, we can see that the vectorizer has analyzed the trip-count of the original loop (1024), along with a canonical induction-variable (&lt;mark&gt;%iv&lt;/mark&gt;), and emitted a &lt;mark&gt;CANONICAL-INDUCTION&lt;/mark&gt; recipe along with a &lt;mark&gt;SCALAR-STEPS&lt;/mark&gt; that increments it by &lt;mark&gt;VF&lt;/mark&gt; to account for the fact that the vectorized loop has width VF, and executes 1024 / VF times. This means that certain instructions must either be width VF, or duplicated VF number of times with different steps for correctness: in our snippet, we see that loads and stores were widened, along with the intrinsic &lt;mark&gt;fmuladd&lt;/mark&gt;. &lt;mark&gt;getelementptr&lt;/mark&gt; should not be widened: it should add VF times the original offset to the base pointer, and this is indeed what is done. For instructions that should not be widened, we see an &lt;mark&gt;EMIT&lt;/mark&gt;/&lt;mark&gt;CLONE&lt;/mark&gt;, or &quot;replicate&quot; recipes. This is also where the interleaving-factor would have come in, in a different example, when something that needs to be widened couldn&#39;t be widened.
Notice that the final line, &lt;mark&gt;branch-on-count&lt;/mark&gt;, is predicated on an abstract vector-trip-count: since the vectorizer could make different decisions about VF and UF as it plans, the vector-trip-count is only materialized after a decision is made.
We see some cryptic &lt;mark&gt;vector-pointer&lt;/mark&gt; recipes, but these will be optimized out by the time we get to the final VPlan. Also notice a minor detail in the operands of &lt;mark&gt;fmuladd&lt;/mark&gt;: the first operand is &lt;mark&gt;%a&lt;/mark&gt;, which is a scalar, while the other two operands, &lt;mark&gt;%load.x&lt;/mark&gt; and &lt;mark&gt;%load.y&lt;/mark&gt;, are widened; if the first operand isn&#39;t fixed up, vectorization of the entire loop would be inhibited for correctness reasons.
The final VPlan is produced post optimizations and materializations:
Notice that the first operand of the &lt;mark&gt;fmuladd&lt;/mark&gt; is now a broadcast of &lt;mark&gt;%a&lt;/mark&gt;, which is essentially a way to fill a vector with a scalar. It lowers to the following LLVM IR:
At this point, the VPlan is &quot;executed&quot; to produce the final LLVM IR. The full example can be seen on &lt;a href=&quot;https://godbolt.org/z/Ga3E4b7jT&quot;&gt;Godbolt&lt;/a&gt;.
VPlan allows us to focus on semantic details of the vectorization, and operate at an abstraction level that is easier to inspect and optimize, and less error-prone, than concrete LLVM IR.</summary>
    <title>The Vectorization-Planner (VPlan) in LLVM</title>
    <updated>2025-09-19T20:59:39+01:00</updated>
    <dc:date>2025-09-19T20:59:39+01:00</dc:date>
  </entry>
  <entry>
    <id>logic/equality</id>
    <link href="logic/equality"/>
    <summary>Here, we talk about equalities, and provide illustrative examples in vanilla Rocq, SProp, and HoTT.
Equality in Mathematics
In &lt;a href=&quot;/logic/zfc&quot;&gt;zfc-based mathematics&lt;/a&gt;, say in abelian groups, $A \oplus B \simeq B \oplus A$, where the equality is a set-based equality. In mathematics based on category theory, equality is too strong a notion on objects, and only speak about objects &quot;upto unique isomorphism&quot;; morphisms in a $\mathrm{1}$-category can be equal though. In higher categories, and in particular, homotopy theory, we talk about &quot;weak homotopy equivalences&quot; almost fully replacing equality. However, it can be tricky to mechanize a theory based on $\infty$-categories; the best dependent-type-theory model we have is homotopy type theory, commonly referred to as HoTT, which we will discuss in the last section.
Universes in Rocq
First, let us briefly talk about the cumulative universe of Rocq. &lt;mark&gt;Prop&lt;/mark&gt; is a &lt;mark&gt;Set&lt;/mark&gt; that can be promoted to &lt;mark&gt;Type&lt;/mark&gt; seamlessly. The reason for the distinction between &lt;mark&gt;Prop&lt;/mark&gt; and &lt;mark&gt;Set&lt;/mark&gt; is an engineering one: &lt;mark&gt;Prop&lt;/mark&gt; is impredicative, while &lt;mark&gt;Set&lt;/mark&gt; is not, and proofs are erased during extraction.
There is an $\infty$ hierarchy within the &lt;mark&gt;Type&lt;/mark&gt; universe, and types of Types are Types themselves.
Inhabitants of a &lt;mark&gt;Set&lt;/mark&gt; are sets of primitive things like $\mathbb{N}$ and $\mathbb{R}$, while inhabitants of a &lt;mark&gt;Prop&lt;/mark&gt; are propositions, which could be $\top$, $\bot$, or some arbitrary term, the inhabitant of which acts as the proof.
Propositions
There are two kinds of equalities in vanilla Rocq. The difference is as follows: propositional equality roughly translates to &quot;requires proof obligation to be discharged by the user&quot;, whereas definitional equality is a simple syntactic rewriting in the metatheory. A propositional equality between propositions can be formalized as:
We then get a proof obligation which we discharge using the axiom:
In the general case, two different inhabitants of &lt;mark&gt;Prop&lt;/mark&gt; are unequal. There is, however, a propositional equality existing between two equal propositions:
where &lt;mark&gt;eq_refl&lt;/mark&gt; is simply the sole inhabitant of an Inductive:
The proof-irrelevant SProp
There are three inhabitants of &lt;a href=&quot;https://coq.inria.fr/refman/addendum/sprop.html&quot;&gt;SProp&lt;/a&gt;: &lt;mark&gt;sUnit&lt;/mark&gt; corresponding to $\top$, &lt;mark&gt;sEmpty&lt;/mark&gt; corresponding to $\bot$, and &lt;mark&gt;sProposition&lt;/mark&gt; corresponding to a definitionally proof-irrelevant term. The way &lt;mark&gt;SProp&lt;/mark&gt; implements definitional proof-irrelevance is a simple engineering detail: there is hard-coding in Rocq to render two inhabitants of &lt;mark&gt;sProposition&lt;/mark&gt; trivially inter-convertible.
Unfortunately, &lt;mark&gt;=&lt;/mark&gt; doesn&#39;t work as expected:
This is because the &lt;mark&gt;SProp&lt;/mark&gt; universe is disjoint from the &lt;mark&gt;Prop&lt;/mark&gt; universe:
Fortunately, we can do better.
Mere propositions in homotopy type theory
In &lt;a href=&quot;https://github.com/HoTT/HoTT&quot;&gt;HoTT&lt;/a&gt;, there is just one $\infty$-ladder:
$\mathbb{1}$, and anything that&#39;s contractible to it, is the ($\mathrm{-2}$)-truncated Type.
A &quot;mere proposition&quot;, an inhabitant of &lt;mark&gt;hProp&lt;/mark&gt;, is simply a ($\mathrm{-1}$)-truncated Type.
&lt;mark&gt;hSet&lt;/mark&gt; is the $\mathrm{0}$-truncated Type.
The notion of truncation is central to HoTT, where $A : \mathrm{Type}_n$ can be truncated to a $\mathrm{Type}_m$, whence higher-than-$m$ morphisms are rendered uninteresting. Any two hProps are propositionally equal:
Conceptually, this is as simple and elegant as &lt;mark&gt;PropEquality&lt;/mark&gt;.
Big thanks to &lt;a href=&quot;https://kenji.maillard.blue&quot;&gt;Kenji Maillard&lt;/a&gt; for the scintillating discussion, and for helping with the examples.
Kudos to Hugo Moeneclaey for giving a wonderful introduction to truncation in the last HoTT class.
Thanks to &lt;a href=&quot;https://www.pédrot.fr&quot;&gt;Pierre-Marie Pédrot&lt;/a&gt; for pointing out some very serious errors.</summary>
    <title>Equality in Mechanized Mathematics</title>
    <updated>2020-02-15T18:12:59+01:00</updated>
    <dc:date>2020-02-15T18:12:59+01:00</dc:date>
  </entry>
  <entry>
    <id>logic/fom</id>
    <link href="logic/fom"/>
    <summary>Starting from the question of what consequence Gödel&#39;s incompleteness theorem has on the foundations of mathematics, we argue for new foundations, and build some intuition for these new foundations in Coq.
Mathematics can be thought of as a game where you start with some objects (say, sets), some axioms on the objects (typically, ZFC), and use a calculus of logical deductions (such as classical logic) to prove things. To mechanize this game in a proof assistant, one would either use a proof-search or tactic system, and check subject reduction. However, mechanized mathematics is done differently from classical mathematics, and we first investigate whether building new foundations is justifiable.
Gödel&#39;s second incompleteness theorem states that any formal system, that can express elementary arithmetic, cannot prove its own consistency, assuming à priori that the system is consistent. It doesn&#39;t destroy the platonic view of mathematics as such, but does destroy the view that there is only one correct foundation for mathematics. Further, one would argue that it is precisely this result that makes mathematics exciting: we can keep rewriting foundations to better suit our abstract intuitions, and build higher abstractions, without worrying about whether there is &quot;one true foundation&quot;.
Departing from ZFC foundations
Some mathematicians would then argue that we&#39;ve settled on ZFC and classical logic as foundations, but this is only true of $\sim$80% of recent mathematical literature. Category theory, in particular, breaks away from set-theoretic foundations; objects and morphisms aren&#39;t sets. Indeed, set-theoretic questions about categories are ill-formed, although one can encode boolean algebras using topoi.
Moreover, one should aim for a foundation that is conducive to mechanization, so proofs can be checked for correctness in an automated fashion. The strategy that Voevodsky et al. used was to extend Martin-Löf type theory, and build a branch of mechanized mathematics, homotopy type theory, which we will refer to henceforth as &lt;a href=&quot;https://github.com/HoTT/HoTT&quot;&gt;HoTT&lt;/a&gt;. It is formalized in Coq, whose foundations are based in Martin-Löf Type Theory.
Preliminaries
What the following proposition says, is that, for every inhabitant of $\mathbb{N}$ that you supply, $f$ will supply an inhabitant of $\mathbb{1}$:
The proof is witness that we can always supply an inhabitant of the type $\mathbb{N} \rightarrow \mathbb{1}$. By Curry-Howard isomorphism:
In type theory, a function is just a non-dependent version of the $\Pi$ type, where the codomain $B$ is fixed:
The logical connective $\neg$, which we will use in the next section, has the following definition in type theory:
Two intuitions in classical mathematics broken
Our first example is the classic proof-by-contradiction. A proof in constructive mathematics cannot proceed in the following logical sequence for a proposition $P$:
In other words,
This is in line with the general philosophy of constructive mathematics, where non-existence of an inhabitant of a type cannot be proved; to supply a witness is to supply a construction of the inhabitant, and indeed, no such construction might exist. Nevertheless, an axiom of excluded middle can be axiomatized and used to construct-via-contradiction, just like any other axiom:
To understand this better, let us take the example of the constructive equivalent of $\mathbb{0} \Rightarrow \textrm {anything}$ in classical mathematics:
Since $\mathbb{0}$ has no inhabitants, the construction of any inhabitant of any type can be proved from it.
It is perhaps prudent to supply another example of a classical statement to illustrate Coq&#39;s type-checker:
In category theory, $\mathbb{0}$ would be the initial object, and there is at most one arrow from the initial object to every other object in the category; $\mathbb{1}$ would be the final object, and there is at most one arrow from every other object in the category to it. The above theorem hypothesizes the existence of a morphism from the final object to the initial object, which of course, cannot exist.
For the second example, let us investigate the classical notion of proof irrelevance. In ZFC, sets and propositions are in different universes of discourse, as opposed to type theory, where proofs objects are first-class. Over the course of its development, the question of proof relevance was raised in type theory. In the following example, $p$ and $q$ are inhabitants of the type $x = y$, and the question is whether $p$ is different from $q$:
The answer, as supplied by HoTT, is: yes, proofs are relevant, and $p$ is different from $q$; a proof is viewed as a homotopy path between two homotopy types, and there is a notion of homotopy equivalence between homotopy paths. In classical mathematics, $p$ would always be equal to $q$.</summary>
    <title>An inquiry into the Foundations of Mathematics</title>
    <updated>2020-02-02T11:11:41+01:00</updated>
    <dc:date>2020-02-02T11:11:41+01:00</dc:date>
  </entry>
  <entry>
    <id>logic/leancoq</id>
    <link href="logic/leancoq"/>
    <summary>Both Lean and Rocq are proof assistants based on pCIC. Here, we argue that what sets them apart are, in essence, cultural differences.
Lean has much lower startup-cost for pure mathematicians, since its built-in features and &lt;a href=&quot;https://github.com/leanprover-community/mathlib&quot;&gt;math library&lt;/a&gt; are great for doing undergraduate-level group theory &amp;amp; topology, masters-level commutative algebra &amp;amp; category theory, but it plateaus quickly thereafter.
Lean seems to have taken a top-down approach, by focusing on writing real proofs as quickly as possible, without compromising on &lt;a href=&quot;https://github.com/digama0/lean-type-theory/releases/download/v1.0/main.pdf&quot;&gt;soundness&lt;/a&gt;. There are three axioms in Lean: &lt;mark&gt;propositional extensionality&lt;/mark&gt;, &lt;mark&gt;quotient soundness&lt;/mark&gt;, and &lt;mark&gt;choice&lt;/mark&gt;; however, these don&#39;t block computation, since computation is done in a VM. They do, however, break good type theoretic properties like &lt;mark&gt;strong normalization&lt;/mark&gt;, &lt;mark&gt;subject reduction&lt;/mark&gt;, and &lt;mark&gt;canonicity&lt;/mark&gt; — this was a conscious design choice.
Rocq, on the other hand, has always been very particular about sound type theoretic foundations. The recent &lt;a href=&quot;https://www.irif.fr/~sozeau/research/publications/drafts/Coq_Coq_Correct.pdf&quot;&gt;&quot;Coq Coq Correct!&quot;&lt;/a&gt; formalizes the Rocq engine proving metatheoretic properties about it, in Rocq, and &lt;a href=&quot;https://github.com/HoTT/HoTT&quot;&gt;HoTT&lt;/a&gt; is being actively developed to fix many of Rocq&#39;s issues.
Working mathematicians have only just started using software for their work, and they often rely on unverified (and sometimes proprietary) tools like magma, sage, and mathematica. Lean is a big step-up, and the good type theoretic properties preserved by Rocq don&#39;t seem as important to them.
Lean supports both constructive and classical reasoning, but quotienting in Rocq is painful due to &lt;a href=&quot;http://www.cs.nott.ac.uk/~psztxa/talks/types-17-hell.pdf&quot;&gt;&quot;setoid-hell&quot;&lt;/a&gt;; something that would be fixed when types are modeled as $\infty$-groupoids instead of setoids, as in the HoTT line of work.
&lt;a href=&quot;https://github.com/leanprover-community/mathlib/blob/master/docs/tutorial/Zmod37.lean&quot;&gt;Quotient-reasoning&lt;/a&gt; makes formalizing commutative algebra painless, and it&#39;s baked into the Lean kernel. However, quotients are tricky to implement without breaking certain metatheoretic properties that Rocq&#39;ers cherish; nevertheless, there is an &lt;a href=&quot;https://math-comp.github.io/htmldoc/mathcomp.ssreflect.generic_quotient.html&quot;&gt;implementation&lt;/a&gt; in Rocq&#39;s &lt;a href=&quot;https://github.com/math-comp/math-comp&quot;&gt;math-comp&lt;/a&gt;, albeit, without a reduction rule.
Lean&#39;s &lt;a href=&quot;https://leanprover.github.io/theorem_proving_in_lean/structures_and_records.html#inheritance&quot;&gt;inheritance&lt;/a&gt; is another good feature; it disallows diamond-inheritance, and seems like a bit of a hack, but Rocq is definitely missing some form of a well-thought-out ad-hoc polymorphism.
Category theory in Lean has not been &lt;a href=&quot;https://github.com/leanprover-community/mathlib/blob/master/docs/theories/category_theory.md&quot;&gt;developed&lt;/a&gt; with higher categories in mind; I&#39;m not sure how one would define $\infty$-categories, since universes are explicit, and since there are no coinductive types: Rocq&#39;s cumulativity seems to have been a better design choice.
One would think that Lean is an engineering feat, since it&#39;s written in C++: Rocq&#39;s math-comp (90k loc) compares to Lean&#39;s mathlib (150k loc); math-comp builds in under ten minutes, while mathlib takes over thirty minutes to build! Indeed, due to their design decision to use a VM for computations, computation happens at a speed comparable to Python-bytecode evaluation — they seem to be overhauling this in Lean4 though, by compiling Lean code down to C before execution.
Another reason for the relative slowness of Lean is that they do a lot of automation under the hood, figuring out, for instance, that when the user writes $X \times Y$, $X$ and $Y$ are topological spaces, and that $\times$ here means product topology.
&lt;a href=&quot;https://github.com/digama0/mm0&quot;&gt;Metamath Zero&lt;/a&gt; might be of interest to readers of this article.
This article has developed over several discussions on coq-club and Lean Zulip.</summary>
    <title>Lean versus Rocq: The cultural chasm</title>
    <updated>2020-01-04T15:38:56+01:00</updated>
    <dc:date>2020-01-04T15:38:56+01:00</dc:date>
  </entry>
  <entry>
    <id>logic/predicativity</id>
    <link href="logic/predicativity"/>
    <summary>Today, we write a quick specialized note on what impredicativity exactly means, for those reasonably familiar with the Rocq syntax.
Historically speaking, Rocq started out with making &lt;mark&gt;Set&lt;/mark&gt; impredicative and they still carry around the flag &lt;mark&gt;--set-impredicative&lt;/mark&gt; to maintain impredicativity in Sets. Let&#39;s check it quickly:
We use a little macro to check the impredicativity of various types, since an equivalent definition wouldn&#39;t type-check:
All inhabitants of &lt;mark&gt;Prop&lt;/mark&gt; are of type Prop, for good reason.
Attempt to check the predicativity of &lt;mark&gt;Type&lt;/mark&gt; naïvely:
The previous example was misleading, and we clarify it in the next example:
The Type universe is cumulative, not impredicative. We show the cumulativity using algebraic universe notation:
&lt;a href=&quot;https://github.com/SkySkimmer&quot;&gt;Gaëtan Gilbert&lt;/a&gt;: intricacies of universes in Rocq
&lt;a href=&quot;http://pauillac.inria.fr/~herbelin/&quot;&gt;Hugo Herbelin&lt;/a&gt;: syntactic mastery over Rocq</summary>
    <title>Predicativity in Rocq</title>
    <updated>2020-03-08T07:27:41+01:00</updated>
    <dc:date>2020-03-08T07:27:41+01:00</dc:date>
  </entry>
  <entry>
    <id>logic/zfc</id>
    <link href="logic/zfc"/>
    <summary>Using a mélange of mathematical syntaxes, we mechanize some exercises from Tao&#39;s excellent book
Overture
Let us informally define sets and set membership $\in$. A set is an unordered collection of &lt;mark&gt;objects&lt;/mark&gt;, with the set itself being an object. Set membership checks whether a given object is contained within a set:
The &lt;mark&gt;axiom of substitution&lt;/mark&gt; uses the definition of equality:
The axiom of substitution is preserved for all sets only defined in terms of $\in$ and $=$.
Thanks to &lt;a href=&quot;https://sites.google.com/view/chaitanyals&quot;&gt;Chaitanya Leena Subramanium&lt;/a&gt; for suggesting the exercises and pushing for the use of natural deductions in the last exercise.
Thanks to &lt;a href=&quot;http://pauillac.inria.fr/~herbelin/&quot;&gt;Hugo Herbelin&lt;/a&gt; for patiently going through many proofs and pointing out the errors.</summary>
    <title>ZFC and propositional logic</title>
    <updated>2020-01-18T08:02:23+01:00</updated>
    <dc:date>2020-01-18T08:02:23+01:00</dc:date>
  </entry>
  <dc:date>2025-12-22T09:47:25+00:00</dc:date>
</feed>